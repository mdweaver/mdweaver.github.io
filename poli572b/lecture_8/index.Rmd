---
title: "POLI 572B"
author: "Michael Weaver"
date: "February 28, 2025"
widgets: [bootstrap]
output: 
  revealjs::revealjs_presentation:
    theme: white
    highlight: haddock
---    

```{r setup, include = F}
require(knitr)
require(magrittr)
require(kableExtra)
require(ggplot2)
require(grid)
require(data.table)
require(UsingR)
options("kableExtra.html.bsTable" = T)


acs_data = fread("acs_lawmed.csv")
acs_data[, FEMALE := SEX]
acs_data[, MARST_f := as.factor(MARST)]
set.seed(1234)
acs_data = acs_data[sample(1:nrow(acs_data), size = 10000)]
acs_data[, MALE := abs(FEMALE - 1)]
acs_data[, MEDICINE := 1-LAW]
acs_data[, sex := as.factor(ifelse(FEMALE == 1, 'Female', 'Male'))]

brexit = fread("analysisData.csv")


```

<style type="text/css">
  .reveal h2,h3,h4,h5,h6 {
    text-align: left;
  }
  .reveal p {
    text-align: left;
  }
  .reveal ul {
    display: block;
  }
  .reveal ol {
    display: block;
  }
  .table-hover > tbody > tr:hover { 
  background-color: #696969;
  color: #FFFFFF;
  }
</style>


# Least Squares Continued

--- 

### Objectives

- Recap: Math of Least Squares
- Least Squares $\to$ CEF
- Interpretation of Coefficients
  - predicted values and residuals
  - design matrix $\to$ coefficients
  - dummy variables
  - continuous variables
  - "controlling"
  - non-linearity


--- 

## Recap


---

### Bivariate Regression 

$$\hat{y_i} = b_0 + b_1x_i$$

#### The slope:

$$b_1 = \frac{Cov(x,y)}{Var(x)}$$

#### The Intercept:


$$b_0 = \overline{y} - \overline{x}\cdot b_1$$


---

### Multivariate Least Squares:

Mean and least squares are **orthogonal projection** of $y$ on $X$.

Previously we predicted $Y$ as a linear function of $x$:

$$\hat{y_i} = b_0 + b_1 \cdot x_i$$

Now, we can imagine predicting $y$ as a linear function of many variables:

$$\hat{y_i} = b_0 + b_1 x_1 + b_2 x_2 + \ldots + b_k x_k$$

---


```{r echo = F, warning=F, message=F}
require(MASS)
require(matlib)
require(Matrix)
require(plotly)

x1 <- rep(1,3)
x2 <- c(2,3,6)
y <- c(1,3,15)
M = cbind(x1,x2)
surf = function (M, th = 0, ph = 15) 
{
  if (!inherits(M, "matrix") | !identical(as.numeric(dim(M)), 
                                          c(3, 2))) 
    stop("M must be a 3 by 2 matrix")
  if (Matrix::rankMatrix(M) == 1) 
    stop("Columns are dependent")
  ec = matlib::echelon(cbind(t(M), c(0, 0)), fractions = T)[, 
  ]
  
  x1 = 0:5
  x2 = 0:5
  eg = expand.grid(x1, x2)
  x3 = matrix(ec[1, 3] * eg$Var1 + ec[2, 3] * eg$Var2, length(x1)) %>% t
  #ownames(x3) = x1
  #colnames(x3) = x2
  plot_ly() %>% 
    add_surface(x = ~ x1, y = ~ x2, z = ~x3, opacity = 0.5) 
}

y_hat = as.vector(M %*% solve(t(M) %*% M) %*% t(M) %*% y)
e = y - y_hat
surf(M) %>% 
  add_paths(x = ~ c(0,M[1,1]), y = ~c(0,M[2,1]), z = ~c(0,M[3,1])) %>%
  add_paths(x = ~ c(0,M[1,2]), y = ~c(0,M[2,2]), z = ~c(0,M[3,2])) %>%
  add_paths(x = ~ c(0,y[1]), y = ~c(0,y[2]), z = ~c(0,y[3])) %>%
  add_paths(x = ~ c(0,y_hat[1]), y = ~c(0,y_hat[2]), z = ~c(0,y_hat[3])) %>%
  add_paths(x = ~ c(y_hat[1], y_hat[1] + e[1]),
            y = ~ c(y_hat[2], y_hat[2] + e[2]),
              z = ~ c(y_hat[3], y_hat[3] + e[3])
            )


```


---

### Deriving Least Squares

Given $\mathbf{y}$, an $n \times 1$ dimensional vector of all values $y$ for $n$ observations

and $\mathbf{X}$, an $n \times 2$ dimensional matrix ($2$ columns, $n$ observations). We call this the **design matrix**. A vector of $\mathbf{1}$ (for an intercept), a vector $x$ for our other variable.

$\mathbf{\hat{y}}$ is an $n \times 1$ dimensional vector of predicted values (for the mean of Y conditional on X) computed by $\mathbf{X\beta}$. $\mathbf{\beta}$ is a vector  $p\times 1$ of (coefficients) that we multiply by $\mathbf{X}$.

We'll assume there are only two coefficients in $\mathbf{\beta}$: $(b_0,b_1)$ so that $\hat{y_i} = b_0 + b_1 \cdot x_i$, so $p = 2$

---

### Deriving Least Squares

$$\mathbf{X} = \begin{pmatrix} 1 & x_1 \\ \vdots & \vdots \\ 1 & x_n \end{pmatrix}; \mathbf{Y} = \begin{pmatrix} y_1 \\ \vdots \\ y_n \end{pmatrix}; \beta = \begin{pmatrix} b_0 \\  b_1 \end{pmatrix}$$ 

---

### Deriving Least Squares


$$\widehat{y_i} = b_0 + b_1 \cdot x_i$$

$$\widehat{y}_{n \times 1} = \mathbf{X}_{n \times p}\beta_{p \times 1}$$

$$\begin{pmatrix} 1 & x_1 \\ \vdots & \vdots \\ 1 & x_n \end{pmatrix} \begin{pmatrix} b_0 \\  b_1 \end{pmatrix} = \begin{pmatrix} 1 \cdot b_0 + x_1\cdot b_1 \\ \vdots \\ 1\cdot b_0 + x_n \cdot b_1 \end{pmatrix} = \begin{pmatrix} \hat{y_1} \\ \vdots \\ \hat{y_n} \end{pmatrix} = \mathbf{\widehat{y}}$$ 

$\mathbf{e} = \mathbf{y} - \mathbf{\hat{y}}$ gives us the residuals (prediction errors).



---

$$\widehat{y_i} = b_0 + b_1 \cdot x_{1i} + b_2 \cdot x_{2i} $$


$$\mathbf{X}\beta = \begin{pmatrix} 1 & x_{11} & x_{21} \\ \vdots & \vdots & \vdots \\ 1 & x_{1n} & x_{2n} \end{pmatrix} \begin{pmatrix} b_0 \\ b_1 \\ b_2 \end{pmatrix} = \begin{pmatrix} \hat{y_1} \\ \vdots \\ \hat{y_n} \end{pmatrix} = \hat{Y}$$


---

### Mathematical Requirements:

1. Matrix $X$ has "full rank"
  - This means that all of the columns of $\mathbf{X}$ are **linearly independent**.
    - cannot have two identical columns
    - cannot have  set of columns that sum up to another column multiplied by a scalar
  - If $\mathbf{X}$ is not full rank, cannot be inverted, cannot do least squares.
    - but we'll see more on the intuition for this later.
    
2. $n \geq p$: we need to have more data points than variables in our equation
  - no longer trivial with multiple regression
  

# Regression and CEF

---

### Conditional Expectation Function

the **conditional expectation function** (Angrist and Pischke)

**expectation**: because it is about the *mean*: $E[Y]$

**conditional**: because it is conditional on values of $X$ ... $E[Y |X]$

**function**: because $E[Y | X] = f(X)$, there is some mathematical mapping between values of $X$ and $E[Y]$.

$$E[Y | X = x] = f(x)$$

---

### Conditional Expectation Function

Another way of thinking about what the **conditional expectation function** is:

$$E[Y | X = x] = \hat{y} | X = x$$

What is our predicted value $\hat{y}$, where $X = x$, such that our prediction of $Y$, $\hat{y}$ has the least error?

- With the CEF, we choose a particular definition of what it means to have the "least error": minimum (Euclidean) distance

---

### Conditional Expectation Function

With least squares, we find a **linear approximation** of the CEF


$$\hat{y_i} = b_0 + b_1x_i$$

$$E[Y | X = x_i] = b_0 + b_1x_i$$

---

We want to understand:

1. What do we mean by "linear"?
2. What is the relationship between $\hat{y}$ and $E[Y|X=x]$?
3. How to generate predicted values
4. Properties of 'residuals' $\mathbf{e}$/prediction errors

---

First, copy this code into R:

```{r echo=T}

earnings = read.csv('https://raw.githubusercontent.com/mdweaver/mdweaver.github.io/refs/heads/master/poli572b/lecture_10/example_7.csv')

```

Data contains:

`AGE`: the age of survey respondents in years

`INCEARN`: the annual earnings (on average) for that age group

`respondents`: Number of survey respondents in this category

---

In pairs:

1. Create the design matrix `X` for this equation $INCEARN_i = b_0 + b_1 AGE_i$ (hint: `cbind`)
2. Using the matrix equation, solve for coefficients $\beta$ in R: (hint: `t()`, `solve()`, `%*%`)
3. Using `lm([your y] ~ [your x], data = [your data])`, solve for coefficients $\beta$ and compare.
4. Using matrix equation, calculate $\hat{y}_i$ (hint: multiply $\mathbf{X}\beta$)
5. Calculate $e$, the residuals using $y$ and $\hat{y}$
5. Plot points for $Y$ by `AGE`; plot a line of $\hat{y}$ by `AGE`: is this a perfect fit of the CEF?


--- 

```{r, echo=F}
cef = acs_data[, list(INCEARN = mean(INCEARN), respondents = .N) , by = AGE]
setkey(cef, AGE)
ggplot(earnings, aes(x = AGE, y = INCEARN)) + geom_point(alpha = 0.75, aes( size = respondents)) + theme_bw() + xlab("Age") + ylab("Earnings (USD)") + ggtitle("CEF of Earnings by Age for Lawyers and Doctors")

```

CEF is not linear

--- 

```{r, echo=F, message=F}
ggplot(cef, aes(x = AGE, y = INCEARN,  weight = respondents)) + geom_point(alpha = 0.75, aes( size = respondents))  + theme_bw() + xlab("Age") + ylab("Earnings (USD)") + ggtitle("CEF of Earnings by Age for Lawyers and Doctors") + geom_smooth(method = 'lm', se = F)

```

Does the regression line fit all values of $E[Y|X=x]$ equally well?

---

### Take aways:

- Least squares is "linear" in that we approximate $Y$ using a linear combination of $X$ (sum $X$ multiplied by coefficients: in the span/column space of $X$)


---

### Graph of Averages

```{r, echo = F, message = F, include = F}
father.son = as.data.table(father.son)
father.son[, bin := round(fheight)]
means = father.son[,list(m = mean(sheight),
                     count = length(fheight)),by=bin]
```

```{r, echo = F, message=F, warning=F}
ggplot(father.son, aes(x = fheight, y = sheight)) + 
  geom_point(alpha = 0.1) + 
    xlab("Father's Height (in)") + ylab("Son's Height (in)") + labs(title = "Son's height by Father's Height") +
   geom_smooth(method = 'lm', se = F, colour = 'red', fullrange = T) + 
  geom_point(aes(x = bin, y = m, size = count), data = means, alpha = 0.5) + theme_bw()
```



---

When we use *individual* data...

```{r}
X = cbind(1, acs_data$AGE)
y = acs_data$INCEARN

beta = solve(t(X) %*% X) %*% t(X) %*% y

beta
```

Do your estimates agree? Why or why not?

---

### Take aways:

- Least squares is "linear" in that we approximate $Y$ using a linear combination of $X$ (sum $X$ multiplied by coefficients: in the span/column space of $X$)
- It is the **best** linear approximation: prediction $\hat{y}$ is closest to $y$ in $n$ dimensional space. It will make **better** predictions where there are more observations with specific value of $X = x$


---

We were not accounting for the **number of people in each value of $X_i$**

```{r}
#With aggregate data...
X = cbind(1, earnings$AGE)
y = earnings$INCEARN
w = earnings$respondents 
w = w * diag(length(w))
beta = solve(t(X) %*% w %*% X) %*% t(X) %*% w %*% y

beta
```

---

### Least Squares and CEF

Least Squares predictions $\widehat{Y} = X'\beta$ gives the linear approximation of $E(Y_i | X_i)$ that has the smallest mean-squared error (minimum distance to the true $E(Y_i | X_i)$).

>- It is "best" linear approximation in that the overall set of predictions $\hat{y}$ has least distance to $y$
>- Least Squares could equivalently fit a line for the mean of Y: $E(Y) | X$ rather than prediction for each $Y_i$. **We could use $E(Y_i | X_i)$ as the dependent variable**, but we need to account for number of observations.

---


### Least Squares and CEF

Alternatively, least squares is a *particular* weighted average of derivative of non-linear CEF (board)

- weights greater closer to median of $x$ 

--- 

```{r, echo=F, message=F}
ggplot(cef, aes(x = AGE, y = INCEARN,  weight = respondents)) + geom_point(alpha = 0.75, aes( size = respondents))  + theme_bw() + xlab("Age") + ylab("Earnings (USD)") + ggtitle("CEF of Earnings by Age for Lawyers and Doctors") + geom_smooth(method = 'lm', se = F)

```

---

Now, in pairs:

1. What is the mean of residuals `e`?
1. Using the `cor` function: get the correlation between residuals `e` and `AGE`


---

### Key facts about regression:

The mathematical procedures we use in regression ensure that: 

$1$. **the mean of the residuals is always zero** (if we include an intercept). Because we included an intercept ($b_0$), and the regression line goes through the point of averages, the mean of the **residuals** is always 0. $\overline{e} = 0$. This is also true of residuals of the mean.

---

### Why?

**the mean of the residuals is always zero**.

We choose $\begin{pmatrix}b_0 \\ b_1 \end{pmatrix}$ such that $e$ is orthogonal to $\mathbf{X}$. One column of $\mathbf{X}$ is all $1$s, to get the intercept (recall how we used vectors to get the mean). So $e$ is orthogonal to $\begin{pmatrix} 1 \\ \vdots \\ 1 \end{pmatrix}$.

$$\mathbf{1}'e = 0$$

And if this is true, the $\sum e_i = 0$ so $\frac{1}{n}\sum e_i = 0$.

---

### Key facts about regression:

The mathematical procedures we use in regression ensure that: 

$2$. $Cov(X,e) = 0$. This is true by definition of how we derived least squares. 

Recall that $Cov(X,e) = \overline{xe}-\overline{x} \ \overline{e}$


We chose $\beta$ ($a,b$) such that $X'e = 0$ so they would be **orthogonal**. 

$X'e = 0 \to \sum x_ie_i = 0 \to \overline{xe}=0$; 

And, from above, we know that $\overline{e}=0$; 

so $Cov(X,e) = \overline{xe}-\overline{x} \ \overline{e} = 0 - \overline{x}0 = 0$.

---

$2$. $Cov(X,e) = 0$. This is true by definition of how we derived least squares. 

This **also means** that residuals $e$ are **always** <u>perfectly uncorrelated</u> (Pearson correlation) with **all** the columns in our matrix $\mathbf{X}$: all the variables we include in the regression model.


# Interpreting Regressions

---

We want to:

- Understand link between design matrix, $\hat{y}$, and coefficients
- Understand dummy variables

---

### Least Squares and Binary Variables

Suppose we fit the following equation using least squares?:

$Earnings_i = b_0$

- What is the design matrix?
- What is the coefficient $b_0$ (solve for it; what is it equivalent to?)
- What is $\hat{y}$ in this case?

---

### Least Squares and Group Means

Suppose we want to estimate earnings as a function of gender

$Earnings_i = b_0 + b_1 \ Female_i$

assuming, as the survey data does, the gender binary

1. What does the design matrix $X$ in this regression look like?
2. What is the interpretation of $\hat{y}$ when $Female_i = 0$?
4. What is the interpretation of $\hat{y}$ when $Female_i = 1$?
3. What does the $b_0$ tell us? 
3. What does the $b_1$ tell us? 

---

```{r}
m1 = lm(INCEARN ~ FEMALE, acs_data)
summary(m1) %>% .$coefficients
```

---

What would change if we changed the design matrix so that instead of a vector of $1$ and  indicated Female ($1$) vs Male ($0$), we used a vector of $2$ and indicated Female ($2$) vs Male ($0$)?

2. What is the interpretation of $\hat{y}$ when $Female_i = 0$?
4. What is the interpretation of $\hat{y}$ when $Female_i = 2$?
3. What does the $b_0$ tell us? 
3. What does the $b_1$ tell us? 

---


### Least Squares and Group Means

$Earnings_i = b_0 + b_1 \ Female_i + \ b_2 Male_i$

1. What does the design matrix $X$ in this regression look like?
2. What is the interpretation of $\hat{y}$ when $Female_i = 0$ and $Male_i = 1$?
4. What is the interpretation of $\hat{y}$ when $Female_i = 1$ and $Male_i = 0$?
2. What does the $b_0$ tell us?
3. What does the $b_1$ tell us?
4. What does the $b_2$ tell us?


```{r}
m2 = lm(INCEARN ~ FEMALE + MALE, acs_data)
```


---

### Least Squares and Group Means

$Earnings_i =  b_0 \ Female_i + \ b_1 Male_i$


1. What does the design matrix $X$ in this regression look like?
2. What is the interpretation of $\hat{y}$ when $Female_i = 0$ and $Male_i = 1$?
4. What is the interpretation of $\hat{y}$ when $Female_i = 1$ and $Male_i = 0$?
2. What does the $b_0$ tell us?
3. What does the $b_1$ tell us?

```{r}
m3 = lm(INCEARN ~ -1 + FEMALE + MALE, acs_data)
```


---

### Least Squares and Group Means

Would the $\hat{y}$ be different in these three models?




--- 

$Earnings_i = b_0 + b_1 \ Female_i $

```{r, }
#Design matrix
model.matrix(m1) %>% head

#Interpret:
coefficients(m1)
```

--- 

$Earnings_i = b_0 + b_1 \ Female_i + \ b_2 Male_i$

```{r}
#Design matrix
model.matrix(m2) %>% head

#Interpret:
coefficients(m2)
```

--- 

$Earnings_i =  b_0 \ Female_i + \ b_1 Male_i$

```{r}
#Design matrix
model.matrix(m3) %>% head

#Interpret:
coefficients(m3)
```


---

### Least Squares and Group Means

Lessons from these examples:

1. Despite different coefficients, least squares can make **identical predictions** $\hat{y}$; changes interpretation of coefficients

2. We need to specify design matrix/model to get estimates of interest. (e.g. difference in means)

3. If we have **exclusive** indicator variables for belonging to distinct categories (sometimes called "dummy variables"), must drop one group if we fit an intercept. (Why?)

4. If we fit group means of $y$: residuals are deviations from group means (centered at 0 within each group). (Why?)

---

### Dummy Variables

If there is an intercept

- **one category must be excluded** ("reference group"); otherwise linear dependence.  (`R` will do this by default)
- coefficient is **difference in means** between the group indicated by the dummy and the intercept ("reference group" mean)

If there is no intercept:

- all categories **must be included**
- separate **intercepts** for each group: coefficient is the mean of the group indicated by the dummy (when all other variables are $0$).

---

### Exercise:

How did different regions in the UK vote on Brexit?

- Download the `brexit` data:

```{r echo=T}

brexit = read.csv("https://raw.githubusercontent.com/mdweaver/mdweaver.github.io/refs/heads/master/poli572b/lecture_10/analysisData.csv")

```

- `Pct_Lev` is the vote share in favor of 'Leaving' the EU
- `Region` is a character indicating the area within the UK.
- Observations are voting districts

---

### Exercise:

Using the `lm` function: (`lm(y ~ x, data = data)`)

1. Tabulate `Region` using `table` to see what values there are.
1. Regress `Pct_Lev` on `Region`
2. Interpret the coefficients. (`summary(your_model)`)
3. (Without looking at them) What would the predicted values $\widehat{Y}$ from this regression be - with respect to the regions?
4. How should we interpret the residuals $e$?
5. Now, run the following:

```{r}
regions = brexit$Region %>% unique %>% sort
brexit$Region2 = factor(brexit$Region, levels = regions[c(12, 1:11)])
```

Repeat the regression. How has the interpretation of coefficients changed?


## Least Squares with Controls

---

### Interpreting w/ Controls

So far, we have considered fitting group means.

- indicator/dummy variables have been for exclusive membership in groups
- what happens if we control for membership in different groups?

---

### Interpreting w/ Controls

We want to estimate differences in earnings across gender, but we want to control for the sector of employment.

In this data, we only have doctors and lawyers, so we can estimate the following:

$Earnings_i = b_0 + b_1 \ Female_i + b_2 \ Medicine_i$

Where $Female_i$ is $1$ if person is female, $0$ if male. $Medicine_i$ is $1$ if they are a doctor $0$ if they are a lawyer.

---

### Interpreting w/ Controls

1. Linear algebra showed us $b_1$ tells us relationship between $Female_i$ orthogonal to $Medicine_i$ and intercept, and $Earnings$

- What is residual $Female_i^*$ in this case? (Think about residuals in models with group dummies above)

>- Residual $Female_i^*$ will be deviation from mean level of $Female$ in law and medicine

---

### Interpreting w/ Controls

2. Slope is $\frac{Cov(x^*, y)}{Var(x^*)}$: Variance of residuals might differ across $Medicine_i$.

- What would happen to $Var(Female^*)$ among doctors if ALL doctors were female?

>- No variation left - no observations differ from the mean of $1$, do not contribute to the slope (zero weight)

---

### Example

Let's now find the (approximate) linear conditional expectation function of earnings, across gender and profession:

```{r}
ls = lm(INCEARN ~ FEMALE + MEDICINE , acs_data)
```

---

### Example

```{r, echo = F}
summary(ls)
```

```{r, echo = F, include = F, message = F, warning=F}
residual_hrswork = lm(FEMALE ~ MEDICINE, acs_data)$residuals
plot_data = rbind(acs_data[, list(INCEARN, FEMALE, MEDICINE)], acs_data[, list(INCEARN, FEMALE, MEDICINE)])
plot_data[, type := factor(rep(c('Observed', 'Residual'), each = nrow(acs_data)))]
plot_data[, FEMALE := as.numeric(FEMALE)]
plot_data[type == 'Residual', FEMALE := residual_hrswork]
```

How do we make sense of, e.g., the slope on `FEMALE`?

---

### Example

```{r, echo = F, warning=F, message=F}
ggplot(plot_data, aes(x = MEDICINE, y= FEMALE)) + geom_point(alpha = 0.01) + geom_smooth(method = 'lm', colour = 'red', se = F) +  geom_smooth(method = 'loess', colour = 'blue', se = F) + theme_bw() + facet_wrap(~type) + ggtitle("Gender Residual on Profession")
```

---

### Example

```{r, echo = F, warning=F, message=F}
ggplot(plot_data, aes(x = FEMALE, y = INCEARN)) + geom_point(alpha = 0.01) + geom_smooth(method = 'lm', colour = 'red', se = F) + theme_bw() + facet_wrap(~type)  + ggtitle("Earnings on Residual Gender")
```




---

### Example

Let's now find the (approximate) linear conditional expectation function of earnings, across gender and hours worked:

We're not controlling for a binary indicator, but a continuous variable.

```{r}
ls = lm(INCEARN ~ FEMALE + UHRSWORK , acs_data)
```

---

### Interpreting w/ Controls

1. Linear algebra showed us $b_1$ tells us relationship between $Female_i$ orthogonal to $Hours_i$ and intercept, and $Earnings$

- What is the residual/orthogonal $Female_i^*$ in this case? ()

>- Residual $Female_i^*$ will have perfectly $0$ covariance/correlation with $Hours_i$

---

### Example

```{r, echo = F}
summary(ls)
```

```{r, echo = F, include = F, message = F, warning=F}
residual_hrswork = lm(FEMALE ~ UHRSWORK, acs_data)$residuals
plot_data = rbind(acs_data[, list(INCEARN, FEMALE, UHRSWORK)], acs_data[, list(INCEARN, FEMALE, UHRSWORK)])
plot_data[, type := factor(rep(c('Observed', 'Residual'), each = nrow(acs_data)))]
plot_data[, FEMALE := as.numeric(FEMALE)]
plot_data[type == 'Residual', FEMALE := residual_hrswork]
```

How do we make sense of, e.g., the slope on `FEMALE`?

---

### Example

```{r, echo = F, warning=F, message=F}
ggplot(plot_data, aes(x = UHRSWORK, y= FEMALE)) + geom_point(alpha = 0.01) + geom_smooth(method = 'lm', colour = 'red', se = F) +  geom_smooth(method = 'loess', colour = 'blue', se = F) + theme_bw() + facet_wrap(~type) + ggtitle("Gender Residual on Hours")
```

Gender and age **orthogonal** but **NOT** *independent*

---

### Interpreting w/ Controls

2. Slope is $\frac{Cov(x^*, y)}{Var(x^*)}$: Variance of residuals might differ across $Hours_i$.

- What happens to $Var(Female^*)$ for values of hours worked where $Hours_i$ badly predicts $Female_i$?

>- Greater variance! Increased weight on those observations

---

### Example

```{r, echo = F, warning=F, message=F}
ggplot(plot_data, aes(x = FEMALE, y = INCEARN)) + geom_point(alpha = 0.01) + geom_smooth(method = 'lm', colour = 'red', se = F) + theme_bw() + facet_wrap(~type)  + ggtitle("Earnings on Residual Gender")
```

---

### Interpreting w/ Controls

What could we do if we really wanted to "hold hours worked constant"? 

- Compare earnings by gender **within** groups where hours are the same  (think about our indicator variables from earlier)?

- We can ensure that gender is exactly unrelated to each year of age by fitting an intercept for each age.


>- In this case, we have `r nrow(acs_data)` observations, so this is doable. Not always so lucky

# Non-Linearity with Least Squares

---

### Non-linearity with Least Squares

If the the CEF, $E[Y|X]$ is not linear in $X$, we can still use least squares to model this relationship.

The easiest choice is to use a polynomial expansion of $X$. 

If a straight-line relationship between $X$ and $Y$ is clearly wrong, we can model a "U"-shape by adding a squared term of $X$:

$Earnings_i = b_0 + b_1 Female + b_2 Hours + b_3 Hours^2$

It is "linear" in that we still multiply values of $X$ by $\beta$ and sum, but we use non-linear transformations of the data.

---


```{r, echo = F, include = F, message = F, warning=F}
residual_hrswork = lm(FEMALE ~ UHRSWORK, acs_data)$residuals
plot_data = rbind(acs_data[, list(INCEARN, FEMALE, UHRSWORK)], acs_data[, list(INCEARN, FEMALE, UHRSWORK)])
plot_data[, type := factor(rep(c('Observed', 'Residual'), each = nrow(acs_data)))]
plot_data[, FEMALE := as.numeric(FEMALE)]
plot_data[type == 'Residual', FEMALE := residual_hrswork]
```


```{r, echo = F, warning=F, message=F}
ggplot(plot_data, aes(x = UHRSWORK, y= FEMALE)) + geom_point(alpha = 0.01) + geom_smooth(method = 'lm', colour = 'red', se = F) +  geom_smooth(method = 'loess', colour = 'blue', se = F) + theme_bw() + facet_wrap(~type) + ggtitle("Residual Gender and Hours (linear)")
```

---


```{r, echo = F, include = F, message = F, warning=F}
residual_hrswork = lm(FEMALE ~ poly(UHRSWORK, degree = 2), acs_data)$residuals
plot_data = rbind(acs_data[, list(INCEARN, FEMALE, UHRSWORK)], acs_data[, list(INCEARN, FEMALE, UHRSWORK)])
plot_data[, type := factor(rep(c('Observed', 'Residual'), each = nrow(acs_data)))]
plot_data[, FEMALE := as.numeric(FEMALE)]
plot_data[type == 'Residual', FEMALE := residual_hrswork]
```


```{r, echo = F, warning=F, message=F}
ggplot(plot_data, aes(x = UHRSWORK, y= FEMALE)) + geom_point(alpha = 0.01) + geom_smooth(method = 'lm', colour = 'red', se = F) +  geom_smooth(method = 'loess', colour = 'blue', se = F) + theme_bw() + facet_wrap(~type) + ggtitle("Residual Gender and Hours (quadratic)")
```

Can we do better?

---


```{r, echo = F, include = F, message = F, warning=F}
residual_hrswork = lm(FEMALE ~ poly(UHRSWORK, degree = 3), acs_data)$residuals
plot_data = rbind(acs_data[, list(INCEARN, FEMALE, UHRSWORK)], acs_data[, list(INCEARN, FEMALE, UHRSWORK)])
plot_data[, type := factor(rep(c('Observed', 'Residual'), each = nrow(acs_data)))]
plot_data[, FEMALE := as.numeric(FEMALE)]
plot_data[type == 'Residual', FEMALE := residual_hrswork]
```


```{r, echo = F, warning=F, message=F}
ggplot(plot_data, aes(x = UHRSWORK, y= FEMALE)) + geom_point(alpha = 0.01) + geom_smooth(method = 'lm', colour = 'red', se = F) +  geom_smooth(method = 'loess', colour = 'blue', se = F) + theme_bw() + facet_wrap(~type) + ggtitle("Residual Gender and Hours (cubic)")
```

---

```{r}
m = lm(INCEARN ~ FEMALE + poly(UHRSWORK, 3, raw = T) , acs_data)
summary(m)$coefficients
```

---

### Non-linearity with Least Squares

We can incorporate all kinds of non-linearity:

- polynomials (up to $n-1$ degrees)
- logarithms (natural log very common)
- exponents
- square-roots

But there are trade-offs...

---

### Non-linearity with Least Squares

1. Choice of non-linearity may be arbitrary:

  - unless we have good theoretical motivation, which non-linearity do we choose?
  - how do we decide which non-linear function is "best"?
  - worst case scenario: we can fit to arbitrary precision.
  
---

Linear fit

```{r, echo = F, include = F, message = F, warning=F}
data(Galton) 
galton = Galton %>% as.data.table() %>%
          .[, list(n = .N), by = list(parent, child)]
w = galton$n/sum(galton$n)
k = 14
galton_s = galton[sample(1:.N, k, prob = w)]
galton_s[, parent := parent + runif(.N, -0.25,25)]
```

```{r echo = F, include = T, message = F, warning=F}
ggplot(galton_s, aes(parent, child)) + 
  geom_point() +
  theme_bw() +
  xlab("Father Height (inches)") + ylab("Child Height (inches)") +
  geom_smooth(method = 'lm', se = F, fullrange = T, col = 'black')  +
  ylim(range(galton$child))

```




---

Perfect Polynomial fit...

```{r echo = F, warning=F, message=F}
m = lm(child ~ poly(parent, degree = k - 1), galton_s)
l = galton_s$parent %>% min %>% `-` (0.01)
u = galton_s$parent %>% max %>% `+` (0.01)
fit = data.frame(parent = seq(l,u,0.001))
fit$child_hat = predict(m, newdata = fit)

ggplot(galton_s, aes(parent, child)) + 
  geom_point() +
  theme_bw() +
  xlab("Father Height (inches)") + ylab("Child Height (inches)") +
  geom_line(data = fit, aes(x = parent, y = child_hat)) +
  ylim(range(galton$child))

```

---

Perfect Polynomial fit?

```{r echo = F, warning=F, message=F}
ggplot(galton_s, aes(parent, child)) + 
  geom_point() +
  theme_bw() +
  xlab("Father Height (inches)") + ylab("Child Height (inches)") +
  geom_line(data = fit, aes(x = parent, y = child_hat))
```


---

### Non-linearity with Least Squares

We risk **overfitting** the data, leading to very bad extrapolations/interpolations.

1. Choice of non-linearity:
  - In general, polynomials not a great idea
  - natural splines fit cubic polynomials in pieces (better)
  - machine learning tools (GAM, KRLS) fit arbitrary functions
  - But we have to cross-validate to assess out-of-sample predictions

---

### Non-linearity with Least Squares

2. Non-linearity makes interpreting coefficients difficult.

  - If we include $x$ and $x^2$, we can't directly interpret these
  - Non linearity means slope is not constant.
  - Coefficients do not summarize the average slope
  - We need to calculate average partial derivative/average partial effect
  
  See Aronow and Miller, Chapter 4.3.4

  Basically, if you non-linearly transform $Y$ or $X$, you need to check how to interpret.
  
---

### Key Takeaways:

- If we 'control' for binary variables, we can exactly "hold things constant".
  - this is only possible when $n >> g$: many more observations than 'groups' we want to hold constant
- If we 'control' for continuous variables, least squares only ensures orthogonality between variable of interest and control.
  - orthogonality $\neq$ independence
  - this ensures only zero linear correlation.
- We can split the difference w/ non-linear transformations
  - how to choose? how to avoid overfitting?
  - difficult to interpret coefficients for transformed variables.



