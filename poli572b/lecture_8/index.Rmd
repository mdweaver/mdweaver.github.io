---
title: "POLI 572B"
author: "Michael Weaver"
date: "March 4, 2019"
widgets: [bootstrap]
output: 
  revealjs::revealjs_presentation:
    theme: black
    highlight: zenburn
---

```{r setup, include = F}
require(knitr)
require(magrittr)
require(kableExtra)
require(ggplot2)
require(grid)
require(data.table)
require(UsingR)
require(lfe)

options("kableExtra.html.bsTable" = T)

acs_data = fread("acs_lawmed.csv")
acs_data[, FEMALE := SEX]
acs_data[, MARST_f := as.factor(MARST)]
#acs_data = acs_data[sample(1:nrow(acs_data), size = 10000)]
acs_data[, MALE := abs(FEMALE - 1)]
acs_data[, sex := as.factor(ifelse(FEMALE == 1, 'Female', 'Male'))]
```

<style type="text/css">
  .reveal h2,h3,h4,h5,h6 {
    text-align: left;
  }
  .reveal p {
    text-align: left;
  }
  .reveal ul {
    display: block;
  }
  .reveal ol {
    display: block;
  }
  .table-hover > tbody > tr:hover { 
  background-color: #696969;
  }
</style>

## Plan for Today

### Review

1. Conditioning

- extrapolation bias
- saturated regression

### Least Squares: Practical Issues

1. Measurement Error

- When we can ignore it
- When we cannot

2. Standard Errors

- Robust Standard Errors
- Clustered Standard Errors
- Bootstrapping


# Review: Conditioning

## Conditioning Pitfalls

### (3) Extrapolation Bias: 

- By forcing relationship between $Z$ (and for generality $Z_1 \ldots Z_k$) and $X$ to take some functional form (usually linear)...
- If cases with different "treatment" also different in $Z$ (there is no variation in treatment within strata), then causal inferences will depend on **extrapolation**
- This extrapolation is often heavily influenced by our mathematical assumptions rather than the underlying data/truth. (King and Zeng)
- Another name for this problem is a **lack of common support**... we don't have otherwise similar cases across different kinds of treatment.

## Extrapolation Bias

```{r, echo = F}
n = 250
z = rnorm(n)
x = rbinom(n, 1 , prob = ifelse(z > 0, 1, 0))

y = 1 + z * 0.25 + x * 1 + x*(z^2)*0.5 + rnorm(n, sd = 0.5)

plot_data = data.table(z,x = as.factor(x),y)
ggplot(plot_data, aes(x = z, y = y, color = x, group = x)) + geom_point() + 
  geom_smooth(method = 'lm', se = F, fullrange = T) + theme_bw()
```

## Extrapolation Bias

### Solutions?

- Restrict analysis to cases where we have overlap in conditioning variables
- Fewer functional form assumptions


## Saturated Regression

- A potential solution to both **extrapolation** and **interpolation** bias

What is it?

- a dummy variable for **every** unique combination of values for conditioning variables.
- we **condition** in exactly the way did last week: stratify the data to compare otherwise **observable** identical cases.
- returns **an** average causal effect, but **variance** weighted (more weight on strata with more variance in "treatment")
- no possibility of **interpolation**
- zero-weight on strata with all "treatment"/all "control" (no **extrapolation**)

## Saturated Regression

Gender bias in earnings: linear conditioning

```{r}
controls_linear = lm(INCEARN ~ FEMALE + AGE + RACE_f + 
             EDUC_f + UHRSWORK + WKSWORK2 + 
             LAW + MARST_f, acs_data)

summary(controls_linear)$coefficients
```

## Saturated Regression

dummy `saturated` = "AGE:RACE_f:EDUC_f:UHRSWORK:WKSWORK2:LAW:MARST_f"

```{r, echo = F}
acs_data[, saturated := paste(AGE, RACE_f, EDUC_f, UHRSWORK, WKSWORK2, LAW, MARST_f, sep = ":")]
acs_data[, var := var(FEMALE), by = saturated]
setkey(acs_data, saturated)

acs_data[var > 0, list(INCEARN, FEMALE, saturated)] %>%
  head(10) %>%
  kable()

```

## Saturated Regression

Gender bias in earnings: saturated regression

```{r, warning=F}
require(lfe)
#felm(y ~ x1 + x2 | fixed_effect | instrument | cluster, data = ___)
controls_saturated = felm(INCEARN ~ FEMALE | saturated | 0 | 0, acs_data)

#Saturated
summary(controls_saturated)$coefficients

#Linear
summary(controls_linear)$coefficients
```

## Saturated Regression

We can use conditioning without **interpolation** bias, **extrapolation** bias, but

- we are still assuming no **omitted variable bias**
- saturated regression suffers from **curse of dimensionality** (a lot of $N$ needed, usually)
- returns a variance-weighted effect, may not be what we want (this is fixable, though)

# Practical Issues: 

# Measurement Error

## Measurement Error

We have previously assumed we measured variables without error

- measurements often imperfect
- if measures are **biased**, estimates will be biased
- if measures have **random error**, it **depends**

## Measurement Error

First, the good news:

> If we have **random** measurement error in $Y$ or $X$, problems are manageable.

## Measurement Error


This is our statistical model; we take all OLS assumptions to be true.

$$Y^* = \alpha + \beta X^* + \epsilon$$

$Y^*$ is the true value of the variable that we observe as $Y$. $E(\delta)=0$, $\delta$ independent of $Y^*$, i.i.d. with variance $\sigma^2_\delta$

$$Y = Y^* + \delta$$

$X^*$ is the true value of the variable that we observe as $X$. $E(\eta)=0$, $\eta$ independent of $X^*$, i.i.d. with variance $\sigma^2_\eta$

$$X = X^* + \eta$$

## Measurement Error

What happens when we have measurement error in $Y$? 
$$Y = \alpha + \beta X^* + \epsilon + \delta$$

But recall: $E(\delta) = 0; Cov(X^*,\delta) = 0$. So, estimate $\widehat{\beta}$ for $\beta$ is **unbiased**.

Standard errors **are affected**:

- Variance Covariance matrix is now:  
- $Var(\epsilon + \delta)(X^{*\prime} X^{*})^{-1}$, not $Var(\epsilon)(X^{*\prime} X^{*})^{-1}$
- Thus, variances will be **larger**, standard errors will be **larger** due to random measurement error.

## Measurement Error

What happens when we have measurement error in $X$? 

$$Y^* = \alpha + \beta (X - \eta) + \epsilon$$
$$Y^* = \alpha + \beta X + \nu$$
Here $\nu = (\epsilon - \beta\eta)$. This produces **bias** in $\widehat{\beta}$, because:

$$cov(X, \nu) = cov(X^* + \eta, \epsilon - \beta\eta) = -\beta \sigma^2_\eta \neq 0$$

## Measurement Error

What happens when we have measurement error in $X$? 

We get **bias**... but what does this bias look like?

$$\small\begin{eqnarray} 
\widehat{\beta} &=& \frac{Cov(X, Y^*)}{Var(X)}    \\
&=& \frac{Cov(X^* + \eta, \alpha + \beta X^* + \epsilon)}{Var(X^* + \eta)} \\
&=& \frac{\beta Cov(X^*, X^*)}{Var(X^*) + Var(\eta)}   \\
&=& \beta \frac{Var(X^*)}{Var(X^*) + Var(\eta)}  \\
\end{eqnarray}$$


## Measurement Error

Attenuation bias!

$$\widehat{\beta} = \beta \frac{Var(X^*)}{Var(X^*) + Var(\eta)}$$

This is $\beta$ times a ratio of variance of true value over that of the observed. (Signal to noise). As the errors $\eta$ increase, this ratio goes toward $0$. So $\widehat{\beta}$ suffers from **attenuation** bias.

Our estimates of $\beta$ will be biased toward **zero**, which may be "conservative".

## Measurement Error

#### The good news:

- Measurement error in $Y$ produces wider standard errors; no bias
- Measurement error in $X$ produces bias toward zero (*for* $X$); (known direction and bounds)
    - true in bivariate, multivariate cases
    
#### The bad news:

If we want the causal effect of $X$ on $Y$ and we must condition on $Z$ to prevet omitted variable bias:

- Measurement error in $Z$ can lead to bias in estimated slope on $X$ ($\widehat{\beta_{X}} \neq \beta_{X}$)
- A problem if $Z$ is really needed for conditioning.

## Measurement Error:

Why does measurement error in $Z$ generate bias in effect of $X$? An intution:

#### Smoking, Drinking, and cirrhosis:

- Heavy alcohol consumption known to cause cirrhosis of the liver.
- We forget this, and study the relationship between smoking and cirrhosis.
- We collect data on 1000 people:
- Say 80% of drinkers have cirrhosis, 10% of non-drinkers.

## Measurement Error:

```{r, echo = F}

data.frame(Smoke = rep(c("Yes", "No"), each = 2),
           Drink = rep(c("Yes", "No"), 2),
           N = c(400, 100, 100, 400),
           `Cirrhosis Rate` = c(0.8, 0.1, 0.8, 0.1)
           ) %>%
kable()
```

If we ignored drinking (omitted variable bias), we would conclude that smoking causes an increase in cirrhosis:

$$\small\begin{eqnarray} 
&=& Cirrhosis_{S} - Cirrhosis_{NS}    \\
&=& (\frac{400}{500}0.8 + \frac{100}{500}0.1) - (\frac{100}{500}0.8 + \frac{400}{500}0.1)\\
0.42 &=& 0.66 - 0.24  \\
\end{eqnarray}$$

## Measurement Error:

Conditioning on drinking removes this spurious correlation:

```{r, echo = F}

data.frame(Smoke = rep(c("Yes", "No"),  2),
           Drink = rep(c("Yes", "No"), each = 2),
           N = c(400, 100, 100, 400),
           `Cirrhosis Rate` = c(0.8, 0.8, 0.1, 0.1)
           ) %>%
kable()
```

Among drinkers, there is no difference between smokers and non-smokers.

>- Why did drinking produce omitted variable bias?

## Measurement Error:

What if we measure smoking perfectly, but we measure drinking very poorly. Chance of miss classification is 40% for both drinkers and non-drinkers. We would end up observing data like this:

```{r, echo = F}

data.frame(Smoke = rep(c("Yes", "No"),  2),
           Drink = rep(c("Yes", "No"), each = 2),
           N = c(280, 220, 220, 280),
           `Cirrhosis Rate` = c(196/280, 64/220, 134/220, 56/280)
           ) %>%
kable()
```

There is still an observed relationship between smoking and cirrhosis after conditioning on drining, because of measurement error of drinking

## Measurement Error

#### The good news:

- Measurement error in $Y$ produces wider standard errors; no bias
- Measurement error in $X$ produces bias toward zero; (known direction and bounds)
    - true in bivariate, multivariate cases
    
#### The bad news:

- Conditioning will fail to remove OVB if conditioning variable measured with error.


# Standard Errors

## Statistical Inference with Least Squares

1. **Model** equation is correctly specified (correct functional forms, all relevant variables are included). $Y_i = \alpha + \beta X_i + \epsilon_i$ is the true data-generating function

2. $\epsilon_i$ are independently, identically distributed (i.i.d.), $E(\epsilon_i) = 0$ and variance of $\sigma^2$.

3. $\epsilon_i$ is **independent** of $X_i$: $X \mathrel{\unicode{x2AEB}} \epsilon$

## We want reliable uncertainty

When estimating effects, we want to know **accurately** how uncertain our estimates are.

- While assumptions 1, 2, and 3 needed to derive OLS standard errors, we are only interested in standard errors if our estimates are not biased. (Sometimes better model specification fixes heteroskedastic errors)
- Assumptions 1 and 3 affect bias
- So, mostly concerned about violations of assumption 2.

## We want reliable uncertainty

How do we deal when 

- errors are non-identically distributed? (heteroskedasticity)
- errors are non-independent? (correlated errors)

## Roadmap

1. Robust Standard errors: address heteroskedasticity
2. Clustered Standard errors: address correlated errors
3. Bootstrapping: for both

## Robust Standard Errors:

These are standard errors (estimates of variance of $\widehat{\beta}$) that are "robust" to **heteroskedasticity**.

In ordinary least squares, we estimate standard errors, by estimating the variance covariance matrix of the regression coefficients: $\sigma^2 (\mathbf{X}'\mathbf{X})^{-1}$. We estimate $\sigma^2$ as $\frac{1}{n-p}\sum e_i^2$.

- but this **assumed** identically distributed errors to solve this mess:

$$cov(\widehat{\beta}|X) = (\mathbf{X}'\mathbf{X})^{-1}\mathbf{X}'E(\epsilon\epsilon'|X)\mathbf{X}(\mathbf{X}'\mathbf{X})^{-1}$$

## Robust Standard Errors:

Robust standard errors are calculated **after** we've estimated the model with least squares; have residuals.

Rather than assume all errors are the same with $Var(\epsilon)$: allow for **arbitrary hetereoskedasticity** (variances differ for every case). 

So we estimate (in its simplest form):

$$Var(\epsilon_i) \approx \widehat{e_i}^2$$

(Draw Matrix on Board)

## Robust Standard Errors:

One major caveat here:

- These are **estimates**, not miracles.
- Trading OLS standard errors for robust SEs is a tradeoff in assumptions.
- And, Angrist and Pischke show that robust SEs can be biased, SOMETIMES SMALLER than OLS SEs.
    - they recommend using the maximum of robust and ordinary SEs.
- Robust SEs are **consistent** as $n \to \infty$. How close to $\infty$ do we need to be? (~300+)
    
## Robust Standard Errors:

Practically:

- there are many varieties (HC0 through HC5)
- generally don't affect standard errors in a large way

In `R`:

- `sandwich` package
- `lmtest` package

## Robust Standard Errors:

Obtaining them in `R`:

1. Estimate your model
2. Plug model into robust variance-covariance estimator (usually `vcovHC` from `sandwich` package)
3. Get diagonal (manually, in "pretty" format using `coeftest` from `lmtest` package)

## Robust Standard Errors:

```{r, echo = F}
veterans = fread('./referenda_vote.csv')
veterans[, Enlist_Percent := veterans/mil_age]
```

```{r}
#1: Estimate OLS / conventional Standard Errors
lm_suff = lm(suff65_yes ~ Enlist_Percent + suff57_yes + state, data = veterans)
summary(lm_suff)
```

## Robust Standard Errors:

```{r, message=F}
require(sandwich)
require(lmtest)
#2: Estimate robust variance covariance matrix:
vcov_robust = vcovHC(lm_suff, type = "HC3")

#3: get pretty report:
coeftest(lm_suff, vcov. = vcov_robust)
```

## Robust Standard Errors:

When do you need them?

- Basically always a good idea to check both conventional and robust standard errors.
- Visual inspection of residuals shows pattern in variance
    - may require changing your model (less linear)
- there are tests, but I never use those in practice.


## Clustered Standard Errors:

These are standard errors that are designed to reflect situations in which errors are not independent of each other across observations:

- Recall PS4: we had you generate data in which errors were drawn for a group G, rather than for each individual
- In reality: recall the experiment by Paluck: villages were assigned to treatment, but villagers were her unit of analysis. Villagers are likely similar to each other by virtue of things happening in that village, so share common "shocks". 

## Clustered Standard Errors:

Ignoring dependence of errors **underestimates** sampling variability of an estimate:

intuition: imagine an experiment where treatments are set at the village level

- if one village has higher potential outcomes, it will make either treatment mean much larger if assigned to treatment; or control mean much larger if assigned to control
- estimated average causal effect is not stable (sampling distribution of difference in means has large variance)

## Clustered Standard Errors:

Extends robust standard errors:

- allows for arbitrary heteroskedasticity
- allow for arbitrary correlation of errors within groups.
- **consistent** as **number of groups** gets larger. Usually needs tens of groups (but see other options in Angrist and Pischke)
- (Draw Matrix on Board)

Unlike robust standard errors:

- Clustered standard errors tend to change the variance estimate substantially.
- The "meat" in the covariance matrix estimator changes (see the board)

## Clustered Standard Errors:

In `R`: many options:

1. `felm`: extends `lm` for panel models and includes clustering options
2. `sandwich`: (finally!) has clustered errors options
3. `multiwayvcov`: built-in tools for clustered errors- best practices.

## Clustered Standard Errors:

```{r, message=F}
require(multiwayvcov)
#2: Estimate robust variance covariance matrix:
vcov_cluster = cluster.vcov(lm_suff, cluster = veterans$state)

#3: get pretty report:
coeftest(lm_suff, vcov. = vcov_cluster)
```

Standard Errors are way smaller (but note, with 2 states, not reliable; certain conditions make clustered errors smaller)

## Clustered Standard Errors:

The big question:

- what level do we cluster at?
    - level at which "treatment" assigned?
    - level at which plausible spatial or temporal autocorrelation?

An alternative:

- aggregate data within clusters: Paluck analysis as difference in village means, not a comparison of individuals within treated and untreated villages.
- Simplifies the analysis, easy to compute conventional standard errors, randomization inference.

## Bootstrapping

Robust/Cluster Standard Errors appeal to **asymptotic** consistency (they only work as $n$ or $groups$ respectively go to infinity)

Bootstrapping can be used to obtain standard errors without making such appeals

- as a result, **sometimes** provide better standard errors with smaller samples.

Bootstrapping still requires researcher choice:

- what is the correct chance process that bootstrapping should emulate?

---

### Heteroskedasticity

**"pairs" bootstrap:**

- Set a number of iterations $k$
- For each iteration, generate new bootstrap data drawing rows from the data **with replacement**
- Estimate your model again, save the coefficient estimates
- Generate sampling distribution of for coefficient estimates
- Take standard deviation, use as standard error

---

### Heteroskedasticity

**"residual" bootstrap:**

- Set a number of iterations $k$
- Compute residuals from your fitted model
- For each iteration, generate new bootstrap data as follows:
    - $e^*$ is drawn at random with replacement from list of residuals (like drawing $\epsilon$)
    - compute $Y^*_i = \widehat{\beta}X_i + e^*$ 
- Estimate your model again, save the coefficient estimates
- Generate sampling distribution of for coefficient estimates
- Take standard deviation, use as standard error


---

### Heteroskedasticity

**"wild" bootstrap:**

- Set a number of iterations $k$
- Compute residuals from your fitted model
- For each iteration, generate new bootstrap data as follows:
    - $e^*_i$ calculated for each by multipling $e_i$ by either $1,-1$ with probability 0.5.
    - compute $Y^*_i = \widehat{\beta}X_i + e^*$ 
- Estimate your model again, save the coefficient estimates
- Generate sampling distribution of for coefficient estimates
- Take standard deviation, use as standard error

---

### Clustering

**"block" bootstrap:**

- Set a number of iterations $k$
- For each iteration, generate new bootstrap data as follows:
    - Draw $G$ groups at random with replacement from list of $G$ clusters in data
    - Combine rows corresponding to randomly sampled groups
- Estimate your model again, save the coefficient estimates
- Generate sampling distribution of for coefficient estimates
- Take standard deviation, use as standard error


## Bootstrapping

In `R`:

1. Program it yourself
2. `sandwich` package: `vcovBS()`
3. `multiwayvcov` packages: `cluster.boot()`
4. Lots of other options

## Bootstrapping

In `R`:

1. Estimate the model
2. Decide correct bootstrap procedure
3. Generate bootstrap sampling distribution for $\widehat{\beta}$
4. Get variances/covariances/standard errors

## Bootstrapping

```{r}
#Get bootstrapped variance covariance matrix
vcov_boot = cluster.boot(lm_suff, cluster = 1:nrow(veterans), 
              boot_type = 'wild')

#3: get pretty report:
coeftest(lm_suff, vcov. = vcov_boot)
```

## Standard Errors Summary:

How much uncertainty about effect sizes?

1. All standard errors require some assumption about the chance process underlying our analysis
2. All assumptions are likely flawed
3. Best to show results are robust to different choices about standard errors
    - particularly important to make sure you don't report the smaller standard errors when other estimates give very different results