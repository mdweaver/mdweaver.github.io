---
title: "POLI 572B"
author: "Michael Weaver"
date: "March 9, 2020"
widgets: [bootstrap]
output: 
  revealjs::revealjs_presentation:
    theme: white
    highlight: haddock
---

```{r setup, include = F}
require(knitr)
require(magrittr)
require(kableExtra)
require(ggplot2)
require(grid)
require(data.table)
require(UsingR)
require(ggdag)
options("kableExtra.html.bsTable" = T)

acs_data = fread("acs_lawmed.csv")
acs_data[, FEMALE := SEX]
acs_data[, MARST_f := as.factor(MARST)]
acs_data[, MALE := abs(FEMALE - 1)]
acs_data[, sex := as.factor(ifelse(FEMALE == 1, 'Female', 'Male'))]
```

<style type="text/css">
  .reveal h2,h3,h4,h5,h6 {
    text-align: left;
  }
  .reveal p {
    text-align: left;
  }
  .reveal ul {
    display: block;
  }
  .reveal ol {
    display: block;
  }
  .table-hover > tbody > tr:hover { 
  background-color: #696969;
  color: #FFFFFF;
  }
</style>

# Inference with Least Squares

--- 

### Objectives

**Recap**

- Ordinary Least Squares assumptions for inference


**Causal Inference**

- Omitted Variable Bias (link to conditioning)
- Interpolation/Extrapolation Bias
- Conditioning in Regression

    
# Recap

---

### Ordinary Least Squares Assumptions:

1. $Y$ is generated by $\mathbf{X\beta} + \epsilon$ (**model** equation is correct)
2. $\epsilon_i$ are independent, identically distributed, with variance $\sigma^2$ for *all* $i$
3. $X_i \perp \!\!\! \perp \epsilon_i$: $X_i$ is **independent** of $\epsilon_i$

With these assumptions:

- $\widehat{\beta}$ is **unbiased** estimator for $\beta$
- known sampling distribution of $\widehat{\beta}$

---

### Least Squares: Statistical Inference

We can also, making assumptions, use regression to estimate, with uncertainty the true value of slope coefficient.

- If the model $Y = \beta X + \epsilon$ where the slopes in $\beta$ are constant and additive for all cases is correct
- If the $\epsilon_i$ are independent and indentically distributed, with mean 0
- If $\epsilon_i$ is **independent** of $X_i$: $X \mathrel{\unicode{x2AEB}} \epsilon$

Then, $\hat{\beta}$ is **unbiased** and has a known sampling distribution that we can estimate from the data.

# A Causal Model for Least Squares

---

### Response Schedule

In the context of **experiments**, we imagined that each observation has potential outcomes corresponding to their behavior under different treatments

- To find the average causal effect of treatment without bias, we had to assume that only treatment status affects the outcome and that treatment is assigned in a manner unrelated to potential outcomes

---

### Response Schedule

In regression, where levels of treatment might be continuous, we generalize this idea to the "response schedule":

- some equation that reflects potential outcomes across different values of the "treatment"

$$Y(D_i = d) = \beta_0 + \beta_1 D_i + \epsilon_i$$
Here $Y_i(D_i = d)$ is the potential outcome for $i$ for a value of $D = d$. 

- this response schedule says that $Y_i$ changes same amount for all unit changes in $D$.
- We only ever observe one $Y_i(D_i = d)$ for the $d$ that occurs, others are counterfactual.

---

### Response Schedule

If we don't know parameters $\beta_0, \beta_1$, what do we need to assume to obtain an estimate $\widehat{\beta_1}$ that we can give a **causal** interpretation? (Unit change in $X$ **causes** $\widehat{\beta}$ change in $Y$)

**We must assume**

- $Y_i$ actually produced according to the response schedule (equation is correctly specified; e.g., linear and additive)
- $D_i$ is chosen at random, so independent of $\epsilon_i$:  $D_i \perp \!\!\! \perp \epsilon_i$. Sometimes we say $D$ is **exogenous**

---

### How does this go wrong?

If the true process generating the data is:

$$Y_i = \beta_0 + \beta_1 D_i + \beta_2 X_i + \epsilon_i$$

with $(D_i,X_i) \perp \!\!\! \perp \epsilon_i$, $E(\epsilon_i) = 0$, $Var(\epsilon_i) = \sigma^2$

What happens when we regress $Y_i$ on a constant and $D_i$ and exclude $X_i$?

---

Because this is a bivariate regression ($Y_i = \widehat{\beta_0} + \widehat{\beta_1} D_i + e_i$)

$$\small\begin{eqnarray} 
\widehat{\beta_1} &=& \frac{Cov(D_i, Y_i)}{Var(D_i)}    \\
&=& \frac{Cov(D_i, \beta_0 + \beta_1 D_i + \beta_2 X_i + \epsilon_i)}{Var(D_i)} \\
&=& \frac{Cov(D_i, \beta_1 D_i)}{Var(D_i)} + \frac{Cov(D_i,\beta_2 X_i)}{Var(D_i)} + \frac{Cov(D_i,\epsilon_i)}{Var(D_i)}   \\
&=& \beta_1\frac{Var(D_i)}{Var(D_i)} + \beta_2\frac{Cov(D_i, X_i)}{Var(D_i)} \\
&=& \beta_1 + \beta_2\frac{Cov(D_i, X_i)}{Var(D_i)}
\end{eqnarray}$$

So, $E(\widehat{\beta_1}) \neq \beta_1$, it is **biased**

---

### Omitted Variable Bias

When we exclude $X_i$ from the regression, we get: 

$$\widehat{\beta_1} = \beta_1 + \beta_2\frac{Cov(D_i, X_i)}{Var(D_i)}$$

This bias is **omitted variable bias**

---

If the truth is:  $Y_i = \beta_0 + \beta_1 D_i + \beta_2 X_i + \epsilon_i$

And $\widehat{\beta_1} = \beta_1 + \beta_2\frac{Cov(D_i, X_i)}{Var(D_i)}$

What is the direction of the bias when:

1. $\beta_2 > 0$; $\frac{Cov(D_i, X_i)}{Var(D_i)} < 0$

2. $\beta_2 < 0$; $\frac{Cov(D_i, X_i)}{Var(D_i)} < 0$

2. $\beta_2 > 0$; $\frac{Cov(D_i, X_i)}{Var(D_i)} > 0$

2. $\beta_2 = 0$; $\frac{Cov(D_i, X_i)}{Var(D_i)} > 0$

2. $\beta_2 > 0$; $\frac{Cov(D_i, X_i)}{Var(D_i)} = 0$

---

### Omitted Variable Bias

This is only source of bias if two things are true:

1. $\beta_2 \neq 0$: omitted variable $X$ has (or is correlated with something that has) an effect on $Y$ 

2. $\frac{Cov(D_i, X_i)}{Var(D_i)} \neq 0$: omitted variable $X$ is correlated with $D$.  

This is why we don't need to include EVERYTHING that might affect $Y$ in our regression equation; **only those variables that affect <u>treatment</u> and <u>the outcome</u>.

---

### Omitted Variable Bias

Link to DAGs:

- Omitted variable bias solved when we "block" backdoor paths from $D$ to $Y$.

Link to conditioning:

1. **Ignorability**/**Conditional Independence** Assumption: within strata of $X$, potential outcomes of $Y$ must be **independent** of cause $D$ (i.e. within values of $X$, $D$ must be as-if random). This is **equivalent** to saying that, within strata of $X$, $\epsilon_i$ are independent of $D_i$, a regression assumption.


---

```{r, echo = F}
dagify(mammogram ~ invite,
       mammogram ~ class,
       mammogram ~ educ,
       educ ~ class,
       death ~ mammogram,
       death ~ class,
       death ~ educ,
       exposure = "invite", 
       outcome = 'death',
       labels = c('mammogram' = "Mammogram", 
                  'death' = 'BC Death',
                  'invite' = "HIP Invite",
                  'class' = 'Class',
                  'educ' = 'Education')) %>%
  ggdag(use_labels = "label", text = F, layout = 'auto')

```

---

### Exercise:

1. Brainstorm an exhaustive list of variables that are causally linked to contracting COVID-19
2. Add these other variables (and any possible backdoor paths) to a DAG that initially includes just this link $\mathrm{Using \ Purell} \xrightarrow{Decreases} \mathrm{COVID-19}$
3. Imagine we want to estimate the causal effect of Purell usage on contracting COVID-19: 
    - which of the variables in your DAG would produce OVB if excluded from the regression?
    - which of the variables in your DAG WOULD NOT produce OVB?
    

# Other Biases

--- 

### (1) Omitted Variable Bias

- Excluding some variable $X$ that should be in the model
- Induces bias if $X$ is causally related to both $D$ and $Y$
- How do we know whether there is some missing $X$ we need to include?
- We need to make a big assumption that we've included all relevant confounding variables: **conditional independence assumption** or **ignorability**

---

###  Potential Pitfalls

Even if we included **all** variables on backdoor path between $D$ and $Y$, we could have problems getting an unbiased estimate of the causal effect using regression:

- we typically assume the model is linear and additive. 
- but the world might be non-linear and interactive (effect of $D$ on $Y$ depends on $Z$)
- our decisions about how to specify the math of regression equation can lead to bias

---

### (2) Interpolation Bias: 

Typically: we approximate the relationship between variables in $X$ and $D$ to be additive and linear. If linear approximation is wrong, we have bias.

- By forcing relationship between $X$ (and for generality $X_1 \ldots X_k$) and $D$ to be linear and additive, conditioning on $X$ may not remove non-linear association between $X$ and $D$, $X$ and $Y$. $\to$ bias.
- This unmodeled relationship will become part of $\epsilon_i$ (because $X$ affects $Y$), and will not be independent of $D_i$ (because there is a non-linear association between $X$ and $D$).

---

### Example: Interpolation

Let's return to the same example we used above: do hours increase earnings? We can use the same American Community Survey data on incomes in law and medicine.

Let's say the **model** we want to estimate is:

$Y_i = \beta_0 + \beta_1 Hours_i + \beta_2 Female_i + \beta_3 Age_i + \epsilon_i$

And we want to find $\beta_1$ with $\widehat{\beta_1}$

---

### Example: Interpolation

```{r}
lm(INCEARN ~ UHRSWORK + SEX + AGE, data = acs_data) %>% summary
```

---

### Example: Interpolation

Questions to ask:

- Is relationship between age and hours, age and income earned **linear**?
- Are the effects of sex and age **additive**? (Relationship between age and hours worked the same for men and women?)

---

### Example: Interpolation

```{r echo = F, include = F, warning=F, message = F}
acs_data[, r_hours := lm(UHRSWORK ~ SEX + AGE, .SD)$residuals]
acs_data[, r_hours_2 := lm(UHRSWORK ~ SEX*AGE, .SD)$residuals]
acs_data[, r_hours_3 := lm(UHRSWORK ~ as.factor(SEX)*as.factor(AGE), .SD)$residuals]
acs_data[, Gender := ifelse(SEX == 1, "Female", "Male")]
plot_data = acs_data[, list(hours = mean(UHRSWORK), inc = mean(INCEARN), 
                r_hours = mean(r_hours), r_hours_2 = mean(r_hours_2),
                r_hours_3 = mean(r_hours_3),
                n = .N), by = list(Gender, AGE)]
```

```{r echo = F, message=F, warning=F}

ggplot(plot_data, aes(x = AGE, y = hours, colour = Gender, group = Gender, weight = n, size = n) ) + geom_point() + geom_smooth() + xlab("Age (years)") + ylab("Mean Hours Worked") + ggtitle("Hours Worked by Age and Gender")

```

---

```{r echo = F, message=F, warning=F}

ggplot(plot_data, aes(x = AGE, y = inc, colour = Gender, group = Gender, weight = n, size = n) ) + geom_point() + geom_smooth() + xlab("Age (years)") + ylab("Mean Annual Income ($)") + ggtitle("Income by Age and Gender")

```

---

```{r echo = F, message=F, warning=F}

ggplot(plot_data, aes(x = AGE, y = r_hours, colour = Gender, group = Gender, weight = n, size = n) ) + geom_point() + geom_smooth() + xlab("Age (years)") + ylab("Mean Residual Hours Worked") + ggtitle("Residual Hours Worked (Assuming Linearity and Additivity)")

```

---

```{r}
lm(INCEARN ~ UHRSWORK + SEX + AGE, data = acs_data) %>% summary
```

---

```{r echo = F, message=F, warning=F}

ggplot(plot_data, aes(x = AGE, y = r_hours_2, colour = Gender, group = Gender, weight = n, size = n) ) + geom_point() + geom_smooth() + xlab("Age (years)") + ylab("Mean Residual Hours Worked") + ggtitle("Residual Hours Worked (Assuming Linearity but NOT Additivity)")

```

---

```{r}
lm(INCEARN ~ UHRSWORK + SEX*AGE, data = acs_data) %>% summary
```

---

```{r echo = F, message=F, warning=F}

ggplot(plot_data, aes(x = AGE, y = r_hours_3, colour = Gender, group = Gender, weight = n, size = n) ) + geom_point() + geom_smooth() + xlab("Age (years)") + ylab("Mean Residual Hours Worked") + ggtitle("Residual Hours Worked (Assuming neither Linearity nor Additivity)")

```

---

```{r}
lm(INCEARN ~ UHRSWORK + as.factor(SEX)*as.factor(AGE), data = acs_data) %>% summary
```

---

### Interpolation Bias:

When we allow for greater **approximation** of the relationship of confounding variables $X$ with $D$ and $Y$ through **linearity** (or any imposed functional form) and **additivity**, we generate **interpolation bias** even if there is no omitted variable bias.

Interpolation bias is relatively easy to diagnose:

- Scatterplot $D$ with $X$
- Choose more flexible model (non-linear/interactive)
- Linear/additive approximation does not often produce massive bias... because least squares is a good linear approximation of non-linear functions **within the region where we have data**.

---

### (3) Extrapolation Bias

Typically: we approximate the relationship between variables in $X$ and $D$ to be additive and linear. 

If $D = 1$ never occurs for certain values of $X$ (e.g. $X > 0$), regression model will use additivity and linearity (or other functional form) to **extrapolate** a predicted value of what $D = 1$ would look like when $X > 0$.

- King and Zeng show that extrapolations are very **sensitive to model choice**: small changes in assumptions create large changes in estimates.

---

### Example: Extrapolation

Does being a democracy cause a country to provide better public goods?

We decide to estimate the following regression model. Let's assume that there is no omitted variable bias.

$Public \  Goods_i = \beta_0 + \beta_1 Democracy_i + \beta_2 ln(Per \ Capita \ GDP_i) + \epsilon_i$

---

Exercise: Download the simulated data

```
if (!require(httr)) install.packages('httr')

 r = GET("https://mdweaver.github.io/poli572b/lecture_9/example_9.csv")
data_9 = as.data.frame(content(r))
data_9$Democracy = as.factor(data_9$Democracy)
```

---

$1$. Estimate $Public \  Goods_i = \beta_0 + \beta_1 Democracy_i + \beta_2 ln(Per \ Capita \ GDP_i) + \epsilon_i$ using the `lm` function. What is the effect of democracy on public goods?
$2$. What does this model assume about the relationship between per capita GDP and public goods for democracies? For non-democracies?
$3$. Plot public goods on GDP per capita, grouping by democracy: 

```
ggplot(data_9, aes(x = l_GDP_per_capita, 
                    y = public_goods, 
                    group = Democracy, color = as.factor(Democracy))) + 
  geom_point() + theme_bw() + theme(legend.position="bottom")
```

$4$. What is the true functional form of the relationship between GDP per capita and public goods for Democracies? For non-democracies where GDP per capita is $> 0$?                      
$5$. Create a new variable for per capita GDP $> 0$.
$6$. Repeat the regression from (1) for cases where $l_GDP_per_capita < 0$. Why is the estimate of $\widehat{\beta_1}$ different?

---

In this example:

We observe no non-democracies with GDP per capita as high as some democracies, but must condition on GDP per capita.

- Using regression ($1$ on previous slide): we linearly and additively **extrapolated** from the model what public goods for non-democracies with high per capita GDP would be (where we have no data, only the model)

or

- Using regression ($6$ on previous slide): we restricted our analysis to the region of the data with common support (range of GDP per capita values that contain both democracies and non-democracies). Linear interpolation, but the model is not extrapolating to regions without data.


# Conditioning with Regression

---

### How to Condition with Regression:

Least Squares can condition while avoiding **interpolation** and **extrapolation** biases:

- Specify model so that it is **inherently** linear: **saturated** regression.

**saturated regression**: dummy variables for each unique combination of values in $\mathbf{X}$, where $\mathbf{X}$ is  a matrix containing all variable needed to make **conditional independence assumption**.



