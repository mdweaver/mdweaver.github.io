---
title: "POLI 572B"
author: "Michael Weaver"
date: "March 14, 2022"
widgets: [bootstrap]
output: 
  revealjs::revealjs_presentation:
    theme: white
    highlight: haddock
---

```{r setup, include = F}
require(knitr)
require(magrittr)
require(kableExtra)
require(ggplot2)
require(grid)
require(data.table)
require(UsingR)
require(ggdag)
require(dplyr)
options("kableExtra.html.bsTable" = T)

acs_data = fread("acs_lawmed.csv")
acs_data[, FEMALE := SEX]
acs_data[, MARST_f := as.factor(MARST)]
acs_data[, MALE := abs(FEMALE - 1)]
acs_data[, sex := as.factor(ifelse(FEMALE == 1, 'Female', 'Male'))]


analysis_df <- 
    readRDS("/home/mdweaver/Dropbox/myanmar-prejudice/02-replication/analysis_df.rds")
```

<style type="text/css">
  .reveal h2,h3,h4,h5,h6 {
    text-align: left;
  }
  .reveal p {
    text-align: left;
  }
  .reveal ul {
    display: block;
  }
  .reveal ol {
    display: block;
  }
  .table-hover > tbody > tr:hover { 
  background-color: #696969;
  color: #FFFFFF;
  }
</style>

# Inference with Least Squares

--- 

### Objectives

**Recap**

- Ordinary Least Squares assumptions for inference


**Causal Inference with Regression**

- Omitted Variable Bias (link to conditioning)
- Interpolation/Extrapolation Bias
- Conditioning in Regression

    
# Recap

---

### Ordinary Least Squares Assumptions:

1. $Y$ is generated by $\mathbf{X\beta} + \epsilon$ (**model** equation is correct)
2. $\epsilon_i$ are independent, identically distributed, with variance $\sigma^2$ for *all* $i$
3. $X_i \perp \!\!\! \perp \epsilon_i$: $X_i$ is **independent** of $\epsilon_i$

If these assumptions are true:

- OLS $\widehat{\beta}$ is **unbiased** estimator for $\beta$
- known sampling distribution of $\widehat{\beta}$


# A Causal Model for Least Squares

---

### Response Schedule

In the context of **experiments**, each observation has potential outcomes corresponding to their behavior under different treatments

- To estimate $ACE$ of treatment without bias, assumed that  treatment status is **independent** of potential outcomes

---

### Response Schedule

In regression, where levels of treatment might be continuous, we generalize this idea to the "response schedule":

- some equation that reflects potential outcomes across different values of the "treatment"
- approximates the "average causal response function":
    - how much potential outcomes of $Y$ change with unit change in $X$, on average


---

### Response Schedule

$$Y(D_i = d) = \beta_0 + \beta_1 D_i + \epsilon_i$$

Here $Y_i(D_i = d)$ is the potential outcome for $i$ for a value of $D = d$. 

- this response schedule says that $Y_i$, on average, changes by $\beta_1$ for a unit changes in $D$ (may average across non-linear effects of $D$)
- We only ever observe one $Y_i(D_i = d)$ for the $d$ that occurs, other values are counterfactual.

---

### Response Schedule

If we don't know parameters $\beta_0, \beta_1$, what do we need to assume to obtain an estimate $\widehat{\beta}_1$ that we can give a **causal** interpretation? (On average, change in $D$ **causes** $\widehat{\beta}_1$ change in $Y$)

**We must assume**

- $Y_i$ actually produced according to the response schedule (equation is correctly specified; e.g., linear and additive)
- $D_i$ is independent of $\epsilon_i$:  $D_i \perp \!\!\! \perp \epsilon_i$. Sometimes we say $D$ is **exogenous**

---

### Response Schedule

Recall OLS assumptions: to have no bias...

- $D$ independent of $\epsilon$
- and $\epsilon$ are other variables that **affect** $Y$, per the model. 
What is one way to be sure we can find effect of $D$ without bias?

>- $D$ randomly assigned

---

### But assumptions are violated

If the true process generating the data is:

$$Y_i = \beta_0 + \beta_1 D_i + \beta_2 X_i + \nu_i$$

with $(D_i,X_i) \perp \!\!\! \perp \nu_i$, $E(\nu_i) = 0$, $Var(\nu_i) = \sigma^2$

What happens when we estimate this model with a constant and $D_i$ but exclude $X_i$?

$$Y_i = \beta_0 + \beta_1 D_i + \epsilon_i$$


---

$$\small\begin{eqnarray} 
\widehat{\beta_1} &=& \frac{Cov(D_i, Y_i)}{Var(D_i)}    \\
&=& \frac{Cov(D_i, \beta_0 + \beta_1 D_i + \beta_2 X_i + \epsilon_i)}{Var(D_i)} \\
&=& \frac{Cov(D_i, \beta_1 D_i)}{Var(D_i)} + \frac{Cov(D_i,\beta_2 X_i)}{Var(D_i)} + \frac{Cov(D_i,\epsilon_i)}{Var(D_i)}   \\
&=& \beta_1\frac{Var(D_i)}{Var(D_i)} + \beta_2\frac{Cov(D_i, X_i)}{Var(D_i)} \\
&=& \beta_1 + \beta_2\frac{Cov(D_i, X_i)}{Var(D_i)}
\end{eqnarray}$$

So, $E(\widehat{\beta_1}) \neq \beta_1$, it is **biased**

---

### Omitted Variable Bias

When we exclude $X_i$ from the regression, we get: 

$$\widehat{\beta_1} = \beta_1 + \beta_2\frac{Cov(D_i, X_i)}{Var(D_i)}$$

This is **omitted variable bias**

- recall: $\beta_1$ is the effect of $D$ on $Y$
- recall: $\beta_2$ is the effect of $X$ on $Y$

---

Excluding $X$ from the model: $\widehat{\beta_1} = \beta_1 + \beta_2\frac{Cov(D_i, X_i)}{Var(D_i)}$

What is the direction of the bias when:

1. $\beta_2 > 0$; $\frac{Cov(D_i, X_i)}{Var(D_i)} < 0$

2. $\beta_2 < 0$; $\frac{Cov(D_i, X_i)}{Var(D_i)} < 0$

2. $\beta_2 > 0$; $\frac{Cov(D_i, X_i)}{Var(D_i)} > 0$

2. $\beta_2 = 0$; $\frac{Cov(D_i, X_i)}{Var(D_i)} > 0$

2. $\beta_2 > 0$; $\frac{Cov(D_i, X_i)}{Var(D_i)} = 0$

---

### Omitted Variable Bias

This only yields bias if two conditions are true:

1. $\beta_2 \neq 0$: omitted variable $X$ has an effect on $Y$ 

2. $\frac{Cov(D_i, X_i)}{Var(D_i)} \neq 0$: omitted variable $X$ is correlated with $D$.(on the same backdoor path)  

This is why we don't need to include EVERYTHING that might affect $Y$ in our regression equation; **only those variables that affect <u>treatment</u> and <u>the outcome</u>.**


---

### Conditioning:

**conditioning** is when we examine the effect of $D_i$ on $Y_i$ *within subsets/strata* of the data defined by the values of $\mathbf{X_i}$, **blocking** backdoor (non-causal) paths from $D$ to $Y$.

- Where $\mathbf{X_i}$ is a set of variables on backdoor paths from $D$ to $Y$.
- We examine relationship between $D$ and $Y$, within groups of cases where values of $\mathbf{X_i}$ are the same (hold value of  backdoor paths constant)


---

### Conditioning Assumptions

$1$. **Ignorability**/**Conditional Independence**: within strata of $X$, potential outcomes of $Y$ must be **independent** of  $D$ 
    - all 'backdoor' paths are blocked; no colliders
    
$2$. **Positivity**/**Common Support**: For **all** values of treatment $d$ in $D$ and all value of $x$ in $X$: $Pr(D = d | X = x) > 0$ and $Pr(D = d | X = x) < 1$
    - There must be variation in treatment **within every strata** of $X$

---

### Omitted Variable Bias

Link to DAGs:

- OVB solved when we "block" backdoor paths from $D$ to $Y$.

Link to conditioning:

- OVB is a result of conditional independence assumption being wrong

Link to OLS assumptions:

- unblocked backdoor paths $\to$ $D$ not independent of $\epsilon$

---

### Exercise:

1. Brainstorm a list of variables that may be causally linked to anti-Rohingya Muslim attitudes in Myanmar (and the direction of the effect)
2. Add these variables (and any backdoor paths) to a DAG that initially includes just this link $\mathrm{Higher \ Income} \xrightarrow{Decreases} \mathrm{AntiMuslim Attitudes}$
3. Imagine we want to estimate the causal effect of income on Anti-Muslim prejudice: 
    - which of the variables in your DAG would produce OVB if excluded from the regression? would it induce upward or downward bias the estimated effect of income?
    - which of the variables in your DAG WOULD NOT produce OVB?


# Other Biases

--- 

### (1) Omitted Variable Bias

- Excluding some variable $X$ that should be in the model
- Induces bias if $X$ is causally related to both $D$ and $Y$ ($X$ is a confounder)
- We need to make a big assumption that we've included all relevant confounding variables: **conditional independence assumption** or **ignorability**


---

###  Potential Pitfalls

Even if we included **all** variables on backdoor path between $D$ and $Y$, we could have problems getting an unbiased estimate of the causal effect using regression:

- we typically assume the model is linear and additive. 
- but the world might be non-linear and interactive 
- our decisions about how to specify the math of regression equation can lead to bias


---

### (2) Interpolation Bias: 

Typically: we approximate the relationship between variables in $X$ and $D$ to be additive and linear. If linear approximation is wrong, we have bias.

- By forcing relationship between $X$ and $D$ to be linear and additive, conditioning on $X$ may not remove non-linear association between $X$ and $D$, $X$ and $Y$. $\to$ bias.
- This unmodeled relationship will become part of $\epsilon_i$ (because $X$ affects $Y$), and will not be independent of $D_i$ (because there is a non-linear association between $X$ and $D$).

---

### Example: Interpolation

Let's return to the same example we used above: do hours increase earnings? 

Let's say the **model** we want to estimate is:

$Y_i = \beta_0 + \beta_1 Hours_i + \beta_2 Female_i +$ $\beta_3 Age_i + \beta_4 Law_i + \epsilon_i$

And we want to find $\beta_1$ with $\widehat{\beta_1}$

- how are we assuming **linearity**?
- how are we assuming **additivity**?

---


### Example: Interpolation

Questions to ask:

- Is relationship between age and hours, age and income earned **linear**?
- Are the effects of sex and age **additive**? (Relationship between age and hours worked the same for men and women?)

---

### Example: Interpolation

```{r echo = F, include = F, warning=F, message = F}
acs_data[, r_hours := lm(UHRSWORK ~ SEX + AGE + LAW, .SD)$residuals]
acs_data[, r_hours_2 := lm(UHRSWORK ~ SEX*AGE + LAW, .SD)$residuals]
acs_data[, r_hours_3 := lm(UHRSWORK ~ as.factor(SEX)*as.factor(AGE) + LAW, .SD)$residuals]
acs_data[, Gender := ifelse(SEX == 1, "Female", "Male")]
plot_data = acs_data[, list(hours = mean(UHRSWORK), inc = mean(INCEARN), 
                r_hours = mean(r_hours), r_hours_2 = mean(r_hours_2),
                r_hours_3 = mean(r_hours_3),
                n = .N), by = list(Gender, AGE)]
```


```{r echo = F, message=F, warning=F}

ggplot(plot_data, aes(x = AGE, y = hours, colour = Gender, group = Gender, weight = n, size = n) ) + geom_point() + geom_smooth() + xlab("Age (years)") + ylab("Mean Hours Worked") + ggtitle("Hours Worked by Age and Gender") + theme_bw()

```


---

```{r echo = F, message=F, warning=F}

ggplot(plot_data, aes(x = AGE, y = inc, colour = Gender, group = Gender, weight = n, size = n) ) + geom_point() + geom_smooth() + xlab("Age (years)") + ylab("Mean Income") + ggtitle("Income by Age and Gender") + theme_bw()

```

---

Assuming additivity and linearity:

```{r}
lm(INCEARN ~ UHRSWORK + SEX + AGE + LAW, 
   data = acs_data) %>% 
  summary
```


---

```{r echo = F, message=F, warning=F}

ggplot(plot_data, aes(x = AGE, y = r_hours, colour = Gender, group = Gender, weight = n, size = n) ) + geom_point() + geom_smooth() + xlab("Age (years)") + ylab("Mean Residual Hours Worked") + ggtitle("Residual Hours Worked (Assuming Linearity and Additivity)") + geom_hline(yintercept = 0) + theme_bw()

```


---

Assuming linearity, not additivity

```{r}
lm(INCEARN ~ UHRSWORK + SEX*AGE + LAW, data = acs_data) %>% summary
```

---

```{r echo = F, message=F, warning=F}

ggplot(plot_data, aes(x = AGE, y = r_hours_2, colour = Gender, group = Gender, weight = n, size = n) ) + geom_point() + geom_smooth() + xlab("Age (years)") + ylab("Mean Residual Hours Worked") + ggtitle("Residual Hours Worked (Assuming Linearity but NOT Additivity)") + geom_hline(yintercept = 0) + theme_bw()

```

---

Assuming neither linearity or additivity

```{r}
lm(INCEARN ~ UHRSWORK + as.factor(SEX)*as.factor(AGE) + LAW, data = acs_data) %>% summary
```

---

```{r echo = F, message=F, warning=F}

ggplot(plot_data, aes(x = AGE, y = r_hours_3 %>% round, colour = Gender, group = Gender, weight = n, size = n) ) + geom_point() + geom_smooth() + xlab("Age (years)") + ylab("Mean Residual Hours Worked") + geom_hline(yintercept = 0) + ggtitle("Residual Hours Worked (Assuming neither Linearity nor Additivity)")

```

---

What is wrong with interpolation bias?

- In conditioning, we want to compare $Y_i(D_i = 1)|X_i$ to $Y_i(D_i = 0)|X_i$ for cases where values of confounding variables $X_i = x$ are the same
- In regression, we compare $Y_i(D_i = 1)|X_i$ against **linear prediction** of $\widehat{Y}(D_i = 0)|X_i$.
- This approximation may fail, sometimes spectacularly...

---

```{r, echo = F, message=F}
n = 100
x = rnorm(n, sd = 2) %>% round
d = ((4 + 0.75*x - 0.5*x^2 + rnorm(n, sd= 2))/3) %>% pnorm %>% rbinom(n = n, size = 1, prob = .)
x[d == 0] = x[d==0] - 0.5
y = 2 + d + -0.5*x+ x^2 + rnorm(n)

m = lm(y ~ d + x)
r_d = lm(d ~ x)$residuals

y_alt = predict(m, newdata = data.frame(d = 1 - d, x = x))
plot_data = data.table(x = x,
           d = d %>% as.factor,
           y = y,
           y_alt = y_alt,
           d_alt = (1 - d) %>% as.factor,
           var_d = r_d^2
           )

dens_data = plot_data[, list(raw = .N/100, adj = (.N/100)*mean(var_d)) , by = x]
dens_data[, adj := adj / sum(adj)]
setkey(dens_data, x)
ggplot(plot_data, aes(x = x, y = y, color = d)) + geom_point(size = 2) + geom_point(aes(x = x, y = y_alt, color = d_alt), alpha = 0.4, size = 3, stroke = 0, stat = 'unique') + theme_bw() + ggtitle("Interpolation Bias") 
```

transparent dots indicate $\widehat{Y}$ interpolated by regression.


---

```{r, echo = F}
require(modelsummary)

m1 = lm(y ~ d + x)
x2 = x^2
m2 = lm(y ~ d + x + x2)

modelsummary(list(m1,m2), gof_omit = 'Log.Lik.|F|AIC|BIC|Adj', stars = T)
```


---


```{r, echo = F, message=F}
n = 100
x = rnorm(n, sd = 2) %>% round
d = ((4 + 0.75*x - 0.5*x^2 + rnorm(n, sd= 2))/3) %>% pnorm %>% rbinom(n = n, size = 1, prob = .)
x[d == 0] = x[d==0] - 0.5
y = 2 + d + -0.5*x+ x^2 + rnorm(n)

m = lm(y ~ d + x)
r_d = lm(d ~ x)$residuals

y_alt = predict(m, newdata = data.frame(d = 1 - d, x = x))
plot_data = data.table(x = x,
           d = d %>% as.factor,
           y = y,
           y_alt = y_alt,
           d_alt = (1 - d) %>% as.factor,
           var_d = r_d^2
           )

dens_data = plot_data[, list(raw = .N/100, adj = (.N/100)*mean(var_d)) , by = x]
dens_data[, adj := adj / sum(adj)]
setkey(dens_data, x)
ggplot(plot_data, aes(x = x, y = y, color = d)) + geom_point(size = 2) + geom_point(aes(x = x, y = y_alt, color = d_alt), alpha = 0.4, size = 3, stroke = 0, stat = 'unique') + theme_bw() + ggtitle("Interpolation Bias") +
geom_line(data = dens_data, aes(x = x, y = raw * 100), inherit.aes = F) + 
  geom_line(data = dens_data, aes(x = x, y = adj * 100), inherit.aes = F, color = 'red') 
```

Actual (black) vs Regression (red) weights

---

### Interpolation Bias:

When we  **approximate** of the relationship of confounding variables $X$ with $D$ and $Y$ through **linearity** (or any imposed functional form) and **additivity**, we may generate **interpolation bias** even if there is no omitted variable bias.

Interpolation bias is relatively easy to diagnose:

- Scatterplot $D$ with $X$
- Choose more flexible model (non-linear/interactive)
- Linear/additive apprx. may be acceptible because least squares is a linear apprx. of non-linear functions **within the region where we have data**.

---

### (3) Extrapolation Bias

Typically: we approximate the relationship between variables in $X$ and $D$ to be additive and linear. 

If $D = 1$ never occurs for certain values of $X$ (e.g. $X > 0$), regression model will use additivity and linearity (or ANY functional form) to **extrapolate** a predicted value of what $D = 1$ would look like when $X > 0$.

- King and Zeng show that extrapolations are very **sensitive to model choice**: small changes in assumptions create large changes in estimates.

---

### Example: Extrapolation

Does being a democracy cause a country to provide better public goods?

We decide to estimate the following regression model. Let's assume that there is no omitted variable bias.

$Public \  Goods_i = \beta_0 + \beta_1 Democracy_i +$ $\beta_2 ln(Per \ Capita \ GDP_i) + \epsilon_i$

---

Download simulated data from here:

[https://pastebin.com/zW4wiqxp](https://pastebin.com/zW4wiqxp)

---

$1$. Estimate $Public \  Goods_i = \beta_0 + \beta_1 Democracy_i + \beta_2 ln(Per \ Capita \ GDP_i) + \epsilon_i$ using the `lm` function. What is the effect of democracy on public goods?

$2$. What does this model assume about the relationship between per capita GDP and public goods for democracies? For non-democracies?

$3$. Plot public goods on GDP per capita, grouping by democracy

```
ggplot(data_9, aes(x = l_GDP_per_capita, y = public_goods, colour = Democracy %>% as.factor)) + geom_point()
```


---

$4$. What is the true functional form of the relationship between GDP per capita and public goods for Democracies? For non-democracies where GDP per capita is $> 0$? 

$5$. Create a new variable for per capita GDP $> 0$.

$6$. Repeat the regression from (1) for cases where `l_GDP_per_capita` $ < 0$. Why is the estimate of $\widehat{\beta_1}$ different?

---

### In this example:

We observe no non-democracies with GDP per capita as high as some democracies, but must condition on GDP per capita.

### Options:

- Using regression ($1$ on previous slide): we linearly and additively **extrapolated** from the model what public goods for non-democracies with high per capita GDP would be (where we have no data, only the model)


---

or

- Using regression ($6$ on previous slide): we restricted our analysis to the region of the data with common support (range of GDP per capita values that contain both democracies and non-democracies). Linear interpolation, but the model is not extrapolating to regions without data.

---

### Conditioning and Regression:

Problems of extrapolation relate to assumption of **Positivity** or **Common Support** in conditioning:

- For conditioning: we assume that we have variation in $D$ within every strata of $X$.
- In regression, absence of common support does not **break** the analysis. We just make model-based extrapolations...
    - in contrast to matching: no common support means no conditioning

---

```{r, echo = F, message = F}
data_9 = fread('./example_9.csv')
ggplot(data_9, aes(x = l_GDP_per_capita,
                    y = public_goods,
                    group = Democracy, color = as.factor(Democracy))) +
  geom_point() + theme_bw() + theme(legend.position="bottom")
```


# Conditioning with Regression

---

### Omitted Variable Bias

#### Solutions?

- No easy fixes
- DAGs, justification
- in the future, will discuss sensitivity tests

---

### Inter/Extrapolation Bias

#### Solutions?

- Restrict analyses to cases where we have overlap in conditioning variables (extrapolation)
- Fewer functional form assumptions (interpolation/extrapolation)

---

### Saturated Regression

One solution to both **extrapolation** and **interpolation** bias is **saturated regression**

- a dummy variable for **every** unique combination of values for conditioning variables $X$.
- we now compare difference in averages of treated/untreated **within** each strata of $X$
- returns **an** average causal effect, but weighted by $N$ and **variance** of $D$ within each strata of $X$. 

---

### Saturated Regression

Pros:

- no possibility of **interpolation**: model is linear and additive in $X$, but "assumption-free" because all unique values of $X$ have their own means. 
- zero weight on strata of $X$ with all "treatment"/all "control" (no **extrapolation**)
    - why is this the case?

---

<small>

```{r, echo = F}
analysis_df = analysis_df %>% 
  mutate(svy_sh_age_rc_n = as.numeric(svy_sh_age_rc),
         svy_sh_education_rc_n = as.numeric(svy_sh_education_rc))

m_naive = lm(svy_sh_prejudice_avg_NAs_kept ~ svy_sh_income_rc, analysis_df)
m_linear = lm(svy_sh_prejudice_avg_NAs_kept ~ svy_sh_income_rc + svy_sh_female_rc + svy_sh_age_rc_n + svy_sh_education_rc_n + svy_sh_ethnicity_rc +  svy_sh_religion_rc + svy_sh_profession_type_rc + svy_sh_income_source_rc, data = analysis_df)

modelsummary(list(naive = m_naive, linear = m_linear), 
             estimate  = "{estimate}{stars} ({std.error})",
             statistic = NULL,
             gof_omit = 'Log.Lik.|F|AIC|BIC|Adj', stars = T
             )
```

</small>

---

### Saturated Regression

```{r}
analysis_df = analysis_df %>% 
  mutate(saturated = paste(svy_sh_female_rc, svy_sh_age_rc, svy_sh_education_rc, svy_sh_ethnicity_rc,  svy_sh_religion_rc, svy_sh_profession_type_rc, svy_sh_income_source_rc, sep = ":"))
```


---

```{r, echo = F}


analysis_df$saturated %>% head(10) %>% kable

```

---

### Saturated Regression

Effect of income on prejudice: saturated regression

```{r, warning=F}
require(fixest)
#feols(y ~ x1 + x2 | dummies , data = ___)
m_saturated = feols(svy_sh_prejudice_avg_NAs_kept ~ svy_sh_income_rc | saturated, analysis_df)
```

---

<small>

```{r, echo = F}
modelsummary(list(naive = m_naive, linear = m_linear, saturated = m_saturated), 
             estimate  = "{estimate}{stars} ({std.error})",
             statistic = NULL,
             gof_omit = 'Log.Lik.|F|AIC|BIC|Adj', stars = T
             )
```

</small>

---

### Saturated Regression

We can use regression for conditioning without **interpolation** bias, **extrapolation** bias, but

- we still make conditional independence assumption
- saturated regression suffers from **curse of dimensionality** (a lot of $N$ needed, usually)
- returns a **variance-weighted effect**, may not be what we want (this is fixable, though)



