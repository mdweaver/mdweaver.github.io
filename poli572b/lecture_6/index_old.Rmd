---
title: "POLI 572B"
author: "Michael Weaver"
date: "February 16, 2024"
widgets: [bootstrap]
output: 
  revealjs::revealjs_presentation:
    theme: white
    highlight: haddock
---

```{r setup, include = F}
require(knitr)
require(magrittr)
require(kableExtra)
require(ggplot2)
require(grid)
require(data.table)
require(UsingR)
options("kableExtra.html.bsTable" = T)
```

<style type="text/css">
  .reveal h2,h3,h4,h5,h6 {
    text-align: left;
  }
  .reveal p {
    text-align: left;
  }
  .reveal ul {
    display: block;
  }
  .reveal ol {
    display: block;
  }
  .table-hover > tbody > tr:hover { 
  background-color: #696969;
  color: #FFFFFF;
  }
</style>

# Least Squares


---

### Objectives

- Recap key ideas on estimating causal effects
- What kind of tools do we need to estimate?
- Why least squares is a good-enough tool

## Introduction:

Seen several approaches to estimating causal effects

>- all causal estimands we have seen are **average** effects: ACE, ATT, etc.

>- all estimands involves comparing **average** outcome in a treated state against **average** outcome in untreated state

---

### Introduction

Estimation of these causal effects involves:

- plugging **average** outcome in some set of untreated cases for the unobserved **average** counterfactual outcome among treated cases
- and vice versa, for ACE

>- to the board

---

### Introduction

We need a tool that lets us **plug in** and compare averages. 

>- Least squares is a powerful, if limited tool to do this.

---

### **Why least squares?**

- We need averages $\to$ LS is a generalization of the mean
- A "simple", yet flexible tool to plug in these values
- Ubiquitous, often "good enough"


---

### **Why least squares?**

In order to use least squares in causal inference, need to understand what the math of regression is doing, as it pertains to:

- given our need for average, how is regression related to the mean?
- how does regression "plug-in" values? 
- how does regression "control" for variables?
- how does math of regression relate back to the causal estimands of interest?

>- As we move from design-based toward model-based inference, we need to be in control of the tool we use.

---

### **Why least squares *math*?**

**No causality**
  
- no effort to prove causality; no assumptions about causal model.

**No parameters/estimands**

- no statistical model; no random variables; no potential outcomes
  
**Regression/Least Squares as Algorithm**

- algorithm with mathematical properties
- without further assumption, plug in data, generate predictions
- limited interpretation

---

### Objectives

**Mechanics** of Bivariate Regression

- the mean (revisited)
- Relationship between variables
    - Covariance; Correlation
- Conditional Expectation Function
- Least Squares 
    - algorithm to get CEF
    - mathematical properties of algorithm
    - **no assumptions**

# The Mean: Revisited

---

**Why do we use the mean to summarize values?**

>- For random variables, it is "best guess" of what value we will draw
>- It is a kind of prediction
>- In what sense is it the "best"?

---

### Squared Deviations

**Why are we always squaring differences?**

**Variance**

$\frac{1}{n}\sum\limits_{i = 1}^{n} (x_i - \bar{x})^2$


**Covariance**

$\frac{1}{n}\sum\limits_{i=1}^n(x_i - \bar{x})(y_i - \bar{y})$

**Mean Squared Error**

$\frac{1}{n}\sum\limits_{i=1}^n(\hat{y_i} - y_i)^2$

---

### Squared Deviations

It is linked to **distance**

What is the distance between two points, $p$ and $q$?

**In $2$ dimensional space**: $(p_1,p_2)$, $(q_1,q_2)$

$$d(p,q) = \sqrt{(p_1 - q_1)^2 + (p_2 - q_2)^2}$$

<br>

**In $k$ dimensional space**: $(p_1,p_2, \ldots, p_k)$, $(q_1,q_2, \ldots ,q_k)$

$d(p,q) = \sqrt{(p_1 - q_1)^2 + (p_2 - q_2)^2 + \ldots + (p_k - q_k)^2}$

---

What is the distance between these two points?

```{r, echo = F}
  vectors = data.frame(x = c(3,0), y = c(0,4))
  ggplot(vectors, aes(x = x, y = y)) + geom_point() +
  coord_fixed() + theme_bw()
```

---

What is the distance between these two points?

$p = (3,0); q = (0,4)$

$$d(p,q) = \sqrt{(p_1 - q_1)^2 + (p_2 - q_2)^2}$$

---

What is the distance between these two points?

$p = (3,0); q = (0,4)$

$$d(p,q) = \sqrt{(p_1 - q_1)^2 + (p_2 - q_2)^2}$$

$$d(p,q) = \sqrt{(3 - 0)^2 + (0 - 4)^2}$$
$$d(p,q) = \sqrt{3^2 + (-4)^2} = \ ?$$

---

What is the distance between these two points?

```{r, echo = F}
  vectors = data.frame(x = c(3,0), y = c(0,4))
  ggplot(vectors, aes(x = x, y = y)) + geom_point() +
  geom_segment(aes(x = c(0,0), y = c(0,0), xend = c(3,0), yend = c(0,4)), 
               arrow = arrow(type = 'closed', length =  unit(0.05, 'npc')), alpha = 0.5) + 
  geom_segment(aes(x = c(3), y = c(0), xend = c(0), yend = c(4)), color = 'red') +
  coord_fixed() + theme_bw()
```


---

### Remember Pythagoras?

What is the distance between two points?

**In $2$ dimensional space**: $(p_1,p_2)$, $(q_1,q_2)$

$$d(p,q) = \sqrt{(p_1 - q_1)^2 + (p_2 - q_2)^2}$$

<br>

**In $k$ dimensional space**: $(p_1,p_2, \ldots, p_k)$, $(q_1,q_2, \ldots ,q_k)$

$d(p,q) = \sqrt{(p_1 - q_1)^2 + (p_2 - q_2)^2 + \ldots + (p_k - q_k)^2}$


## The Mean

One way of thinking about the mean is as a **prediction**. 

We observe many values in $Y$: $y_1 \dots y_n$. We want to choose a **single value**, $\hat{y}$, that is the best prediction for all the values in $Y$.

If we say that the "best" prediction is one with with the shortest distance between the prediction $\hat{y}$ and the actual values of $y_1 \dots y_n$...

then the algorithm that gives us this best prediction is the mean.

- since distances are sums of squared differences, this is why mean minimizes the **variance** of prediction errors ($y_i - \hat{y}$).

---

### Deriving the mean:

Imagine we have a variable $Y$ that we observe as a sample of size $n$. We can represent this variable as a **vector** in $n$ dimensional space.

$$Y = \begin{pmatrix}3 \\ 5 \end{pmatrix}$$

More generally, we imagine our observed data $Y$ is a vector with one dimension for each data point (observation)

---

### What is a vector?

- a **vector** is an array of numbers of length $n$. 

- can be portrayed graphically as an arrow from the origin (the point $(0,0)$ or $(0,0,\dots,0)$) to a point in $n$ dimensional space

- vectors can be added, vectors can be multiplied by a number to extend/shorten their length (each element multiplied by same number)

---

### Deriving the mean:

```{r, echo = F}
  vectors = data.frame(x = c(3,0), y = c(0,5))
  ggplot(vectors, aes(x = x, y = y)) +
  geom_segment(aes(x = c(0), y = c(0), xend = c(3), yend = c(5)), 
               arrow = arrow(type = 'closed', length =  unit(0.05, 'npc'))) + xlim(-1,5) + ylim(-1,6) + 
  coord_fixed() + theme_bw() + xlab('n1') + ylab('n2')
```

---

### Deriving the mean:

We want to pick one number (a scalar) $\hat{y}$ to predict all of the values in our vector $Y$. 

This is equivalent to doing this:

$$Y = \begin{pmatrix}3 \\ 5 \end{pmatrix} \approx \begin{pmatrix}\hat{y} \\ \hat{y} \end{pmatrix} = \hat{y} \begin{pmatrix}1 \\ 1 \end{pmatrix}$$

Multiplying a $(1,1)$ vector by a constant.

- $\hat{y}$ will be on the line that runs through the origin $(0,0)$ and $(1,1)$ 
- Why $(1,1)$?

---

Choose $\hat{y}$ on the blue line at point that **minimizes** the distance to  $y$.

```{r, echo = F}
  vectors = data.frame(x = c(3,0), y = c(0,5))
  ggplot(vectors, aes(x = x, y = y)) +
  geom_abline(aes(slope = 1, intercept = 0), color = 'blue' ) + 
  geom_segment(aes(x = c(0,0), y = c(0,0), xend = c(3,1), yend = c(5,1)), 
               arrow = arrow(type = 'closed', length =  unit(0.05, 'npc'))) + xlim(-1,5) + ylim(-1,6) + 
  coord_fixed() + theme_bw() +
    geom_text(aes(x=3,y=5.2,label='y')) + 
    geom_text(aes(x=1.2,y=1.2, label = "(1,1)")) +
    xlab('n1') + ylab('n2') + theme_bw()
```

---

### Deriving the mean:

$y = \begin{pmatrix}3 \\ 5 \end{pmatrix}$ can be decomposed into two separate vectors: a vector containing our prediction  ($\hat{y}$):

$\begin{pmatrix} \hat{y} \\ \hat{y} \end{pmatrix} = \hat{y} \begin{pmatrix} 1 \\ 1 \end{pmatrix}$

and another vector $\mathbf{e}$, which is difference between the prediction vector and the vector of observations (the **residual** or **prediction error**):

$\mathbf{e} = \begin{pmatrix}3 \\ 5 \end{pmatrix} - \begin{pmatrix} \hat{y} \\ \hat{y} \end{pmatrix}$

---

```{r, echo = F}
ggplot() +
  geom_abline(aes(slope = 1, intercept = 0), color = 'blue' ) + 
  geom_segment(aes(x = c(0), y = c(0), xend = c(3), yend = c(5)), 
               arrow = arrow(type = 'closed', length =  unit(0.05, 'npc'))) + xlim(-1,5) + ylim(-1,6) + geom_segment(aes(x = 3, y = 3, xend = 3, yend = 5),arrow = arrow(type = 'closed', length =  unit(0.05, 'npc')),
               colour = 'red', alpha = 0.5) + 
  geom_segment(aes(x = 0, y = 0, xend = 3, yend = 3),arrow = arrow(type = 'closed', length =  unit(0.05, 'npc')),
               colour = 'green', alpha = 0.5) +
  coord_fixed() + xlab('x') + ylab('y') + 
  geom_text(aes(x = c(2,2,3.5), y = c(4,1.5,4), label = c("y", "y_hat", "e"))) + 
  theme_bw() + xlab('n1') + ylab('n2') 
```

---

### Deriving the mean:

This means our goal is to minimize $\mathbf{e}$, residuals.

How do we find the closest distance? The length of $\mathbf{e}$ is calculated by taking:

$$len(\mathbf{e})= \sqrt{(3-\hat{y})^2 + (5 - \hat{y})^2}$$

**When is the length of $\mathbf{e}$ minimized?** 


>- when angle between $\hat{y} \begin{pmatrix} 1 \\ 1 \end{pmatrix}$ and $\mathbf{e}$ is $90^{\circ}$.

--- 

### Deriving the mean:

```{r, echo = F, message=F, warning=F}
ggplot() +
  geom_abline(aes(slope = 1, intercept = 0), color = 'blue' ) + 
  geom_segment(aes(x = c(0), y = c(0), xend = c(3), yend = c(5)), 
               arrow = arrow(type = 'closed', length =  unit(0.05, 'npc'))) + xlim(-1,5) + ylim(-1,6) + geom_segment(aes(x = 4, y = 4, xend = 3, yend = 5),arrow = arrow(type = 'closed', length =  unit(0.05, 'npc')),
               colour = 'red', alpha = 0.75) + 
  geom_segment(aes(x = 0, y = 0, xend = 4, yend = 4),arrow = arrow(type = 'closed', length =  unit(0.05, 'npc')),
               colour = 'green', alpha = 0.75) +
  coord_fixed() + xlab('x') + ylab('y') + 
  geom_text(aes(x = c(2,2,3.5), y = c(4,1.5,4), label = c("y", "y_hat", "e"))) +
   theme_bw() + xlab('n1') + ylab('n2') 
```

---

### Deriving the mean:

Values $Y$ are sample of size $n$ are represented by a vector in $n$ dimensional space. $\hat{y}$ is a prediction of $y$.

- We choose a value $\hat{y}$ in **one dimensional** sub-space (typically on a line through: $\begin{pmatrix} 1_1 \ 1_2  \ldots  1_n \end{pmatrix}$)

- Such that difference between $\hat{y}$ and $Y$---$\mathbf{e}$, residual---is shortest

---

### Deriving the mean:

What if we have more information about each case $i$? Let's call this information $X$, with $x_1 \dots x_n$.

How can we use this to improve our predictions, $\hat{y}$, of $Y$?

- We could choose a value $\hat{y}$ in **two dimensional** plane (on 2D rectangle that passes through: $\begin{pmatrix} 1_1 \ 1_2  \ldots  1_n \end{pmatrix}$) and $\begin{pmatrix} x_1 \ x_2  \ldots  x_n \end{pmatrix}$)

- Such that difference between $\hat{y}$ and $y$---$\mathbf{e}$, residual---is shortest

>- As we will see, this is least squares

# Digression

---

If we are thinking about how knowing the value of one variable informs us about the value of another... might want to think about correlation.

---

### Covariance and Correlation

How are these variables associated?

```{r, echo = F}
dat = fread("./referenda_vote.csv") %>%
      .[state %in% "IOWA"] %>%
      .[, enlist_rate := veterans/mil_age] %>%
      .[enlist_rate < 0.6] %>%
      .[, suffrage_diff := suff65_yes - suff57_yes]

plot(dat$mil_age, dat$suffrage_diff,
     xlab = "Military Aged-Males (#)",
     ylab = "Change in Pro Black Suffrage Vote (1865-1857)",
     main = "Relationship between Military Aged Males and Suffrage Vote")

```


---

### Covariance and Correlation

How are these variables associated?

```{r, echo = F}


plot(dat$enlist_rate, dat$suffrage_diff,
     xlab = "Union Army Enlistment Rate (%)",
     ylab = "Change in Pro Black Suffrage Vote (1865-1857)",
     main = "Relationship between Enlistment Rate and Suffrage Vote")

```


---

### Covariance and Correlation

**Covariance**

$$Cov(X,Y) = \frac{1}{n}\sum\limits_{i=1}^n(x_i - \bar{x})(y_i - \bar{y})$$

$$Cov(X,Y) = \overline{xy} - \bar{x}\bar{y}$$

>- Divide by $n-1$ for sample covariance.

---

### Covariance and Correlation

**Variance**

Variance is also the **covariance** of a variable with itself:

$$Var(X) =  \frac{1}{n}\sum\limits_{i=1}^n(x_i - \bar{x})^2$$

$$Var(X) = \overline{x^2} - \bar{x}^2$$


---

Covariance of military aged males and suffrage vote:

```{r, echo = T}
x1 = dat$mil_age
y1 = dat$suffrage_diff
mean(x1*y1) - (mean(x1)*mean(y1))
```

Covariance of enlistment rate and suffrage vote:

```{r, echo = T}
x2 = dat$enlist_rate
y2 = dat$suffrage_diff
mean(x2*y2) - (mean(x2)*mean(y2))
```

---

Why is the $Cov(MilAge,Suffrage)$ larger than $Cov(Enlist,Suffrage)$?

```{r, echo = F, warning= F, message=F}
t_dt = dat[, list(mil_age, enlist_rate, suffrage_diff)] %>% as.data.table %>% melt.data.table(measure.vars = c('mil_age', 'enlist_rate', 'suffrage_diff'))

ggplot(t_dt, aes(x = value)) + geom_histogram() + facet_wrap(. ~ variable, nrow = 1, scales = 'free_x') + theme_bw() +
  ggtitle("Histograms")
```

---

Why is the $Cov(Width,Volume)$ larger than $Cov(Width,Height)$?

- Scale of covariance reflects scale of the variables. 

- Can't directly compare the two covariances

---

### Covariance: Intuition

```{r, echo = F, message = F}
require(ggplot2)
require(grid)
#get a data set
X <- dat$enlist_rate
Y <- dat$suffrage_diff

#build the data frame
data <- data.frame(X,Y)

#get the means
X.bar <- mean(X)
Y.bar <- mean(Y)

#sign function the rectangle
data$S <- sign((X-X.bar)*(Y-Y.bar))

data = data[data$S != 0,]
data$S = factor(data$S)

#set ggplot parameters
circle.size = 2
colors = list('red', '#0066CC', '#4BB14B', '#FCE638')

SD_Theme = theme(text = element_text(family='Avenir'),
                 legend.position="none",
                 axis.line = element_line(color='#BFBFBF', size=.5),
                 panel.grid.major = element_line(color='#BFBFBF', size=.25),
                 plot.margin = unit(c(2,2,2,2), "cm"),
                 axis.title = element_text(family='Avenir', face='bold', hjust=.5, vjust=.5, size=12),
                 axis.text = element_text(family='Avenir', color='black'),
                 title = element_text(family='Avenir', face='bold', hjust=.5, vjust=1, lineheight=3150, size=10))


ggplot(data, aes(X,Y, xmin=X.bar, xmax=X, ymin=Y.bar, ymax=Y)) + geom_point(size=circle.size, pch=21) +
  geom_hline(yintercept=Y.bar, linetype='longdash') +
  geom_vline(xintercept=X.bar, linetype='longdash') +
  geom_rect(alpha=.2, aes(fill=S)) +   scale_fill_manual(values=c('red','#0066CC')) +
  ggtitle('Products of deviations from the mean') +
  SD_Theme + xlab("Enlist Rate") + ylab("Suffrage Change")
```

---

### Correlation

#### **Correlation** puts covariance on a **standard scale**

**Covariance**

$Cov(X,Y) = \frac{1}{n}\sum\limits_{i=1}^n(x_i - \bar{x})(y_i - \bar{y})$


**Pearson Correlation**

$r(X,Y) = \frac{Cov(X,Y)}{SD(X)SD(Y)}$

- Dividing by product of standard deviations scales the covariance

- $|Cov(X,Y)| <= \sqrt{Var(X)*Var(Y))}$


---

### Correlation

- Correlation coefficient must be between $-1, 1$

- At $-1$ or $1$, all points are on a **straight line**

- Negative value implies increase in $X$ associated with decrease $Y$.

- If correlation is $0$, the covariance must be?

- If $Var(X)=0$, then $Cor(X,Y) = ?$


---

### Correlation: Interpretation

- Correlation of $(x,y)$ is same as correlation of $(y,x)$

- Values closer to -1 or 1 imply "stronger" association
    
- Correlations cannot be understood using ratios.
    - Correlation of $0.8$ is not "twice" as correlated as $0.4$.
- Pearson correlation agnostic about outliers/nonlinearities
- Correlation is not causation

---

### Practice:

Practice is two-fold: understand the concept and write functions in `R`

```
#name your function: my_mean
#function(x); x is an argument, we can add other arguments
my_mean = function(x) {
  n = length(x) #we'll plug in whatever value of x we give to the function 
  s = sum(x)
  return(s/n) #what value(s) should the function return?
}
```

--- 

### Practice:

Get `data_1` here:  [https://pastebin.com/LeSpqKKk](https://pastebin.com/LeSpqKKk)

Without using `cor()` or `cov()` or `var()` or `sd()` functions in `R`

**hint: generate your OWN functions**

1. Inspect `data_1`
1. Calculate mean of $X$; mean of $Y$
2. Calculate $Var(X)$ and $Var(Y)$
3. Calculate correlation $(X,Y)$


--- 

### Practice:

```{r, echo = F, message = F}
plot_data = as.data.table(anscombe)
plot_data$id = 1:nrow(plot_data)
plot_data = melt.data.table(plot_data, id.vars = 'id', measure = patterns('^x', '^y'), value.name = c('x','y'), variable.name = 'dataset') 

ggplot(plot_data, aes(x, y)) + geom_point() + geom_smooth(method = 'lm', se = F, fullrange = T) + facet_grid(. ~ dataset) + theme_bw()
```

--- 

### Covariance and Correlation

- Both measure linear association of two variables
- Scale is either standardized (correlation) or in terms of products (covariance)
- May be inappropriate in presence of non-linearity; outliers



# Conditional Expectation Function

---

### Generalizing the Mean:

The **mean** is useful...

... but often we want generate better predictions of $Y$, using additional information $X$. 


---

### Generalizing the Mean:

The **mean** is useful...

... but often we want to know if the mean of something $Y$ is different across different values of something else $X$. 

To put it another way: the mean of $Y$ is the $E(Y)$ (if we are talking about random variables). Sometimes we want to know $E[Y | X]$

- In causal inference, we want $E[Y | Z = z]$ or $E[Y | D = d, X = x]$
- differences in means in experiments, plug-in missing counterfactuals in conditioning, plug-in counterfactual trends in DiD

---

### Generalizing the Mean:

We are interested in finding the **conditional expectation function** (Angrist and Pischke)

**expectation**: because it is about the *mean*: $E[Y]$

**conditional**: because it is conditional on values of $X$ ... $E[Y |X]$

**function**: because $E[Y | X] = f(X)$, there is some relationship we can look at between values of $X$ and $E[Y]$.

$$E[Y | X = x]$$

---

### Generalizing the Mean:

Another way of thinking about what the **conditional expectation function** is:

$$E[Y | X = x] = \hat{y} | X = x$$

What is the predicted value $\hat{y}$, where $X = x$, such that $\hat{y}$ has the smallest distance to $Y$?

---

### Generalizing the Mean:

These points emphasize the **conditional** and **expectation** part of the CEF.

The difficulty is: how do we find the function?

>- a function takes some value of $X$ and uniquely maps it to some value of $E[Y]$
>- we need to "learn" this function from the data
>- depending on how much data we have, we have to make some choice of how to interpolate/extrapolate when "learning" this function

---

### Generalizing the Mean:

There are **many ways** to define and estimate the **function** in the  **conditional expectation function**

- one easy choice is to assume that the CEF is **linear**.
- That is to say $E[Y]$ is **linear** in $X$. 
- The **function** takes the form of an equation of a line.
- This leads us to bivariate regression

---

### Equation of a line

```{r, echo = F}
x = -10:100
y = 3 + 2*x
plot(x,y, type ='l', ylim = c(-2,25), xlim = c(-2,10))
abline(h=0,v=0, lty = 2)
```

---

### Equation of a line

```{r, echo = F}
x = -10:100
y = 3 + 2*x
plot(x,y, type ='l', ylim = c(-2,25), xlim = c(-2,10))
abline(h=0,v=0, lty = 2)
segments(x0=2,x1=4, y0 = 3+2*2, y1 = 3+2*2, col = 'red')
segments(x0=4,x1=4, y0 = 3+2*2, y1 = 3+2*4, col = 'red')
text(2,5,"Run = 2")
text(2,3+2*2, pos = 3, "(x1,y1)")
text(5,10,"Rise = 4")
text(4,3+2*4, pos = 3, "(x2,y2)")

```

---

### Equation of a line

<div style="font-size: 150%;">$slope = \frac{rise}{run} = \frac{y2-y1}{x2-x1}$</div>


>- Change in $y$ with a 1 unit change in $x$.


---

### Equation of a line

```{r, echo = F}
x = -10:100
y = 3 + 2*x
plot(x,y, type ='l', ylim = c(-2,25), xlim = c(-2,10))
abline(h=0,v=0, lty = 2)
segments(x0=2,x1=4, y0 = 3+2*2, y1 = 3+2*2, col = 'red')
segments(x0=4,x1=4, y0 = 3+2*2, y1 = 3+2*4, col = 'red')
text(2,5,"Run = 2")
text(5,10,"Rise = 4")
abline(h = 3, col = 'blue')
text(0,3, "Intercept", col = 'blue', pos = 2)
```

---

### Equation of a line

<div style="font-size: 150%;">$intercept = (y | x=0)$</div>


>- Value of $y$ when $x = 0$. Where the line crosses the $y$-axis.

---

### Equation of a line

<div style="font-size: 200%;">$y = intercept + slope*x$</div>

or, by convention:

<div style="font-size: 200%;">$y = a + bx$</div>

## Generalizing the Mean:

### How do we choose the line that best captures:

$$E[Y] = a + b\cdot X$$

or 

$$\hat{y} = a + b\cdot X$$

---

### What line fits this?


```{r, echo = F, message = F, include = F}
data(galton)
```



```{r, echo = F, message = F}
ggplot(father.son, aes(x = fheight, y = sheight)) + 
  geom_point(alpha  = 0.5)  + 
   xlab("Father's Height (in)") + ylab("Son's Height (in)") + labs(title = "Son's height by Father's Height") + theme_bw()
```

---

### Which line?

```{r, echo = F, message = F}
m_x = mean(father.son$fheight)
m_y = mean(father.son$sheight)
sd_x = sd(father.son$fheight)
sd_y = sd(father.son$sheight)
slope = sd_x / sd_y
x_norm = sort((father.son$fheight - m_x)/sd_x)

range = -5:5
line_x = x_norm*sd_x + m_x
line_y = x_norm*slope*sd_y + m_y
sd_line = data.frame(line_x, line_y)

ggplot(father.son, aes(x = fheight, y = sheight)) + 
  geom_point(alpha = 0.5) + 
    xlab("Father's Height (in)") + ylab("Son's Height (in)") + labs(title = "Son's height by Father's Height") +
  geom_line(aes(x = line_x, y = line_y), data = sd_line, colour = 'blue')+ theme_bw()
```

---

### Which line?

```{r, echo = F}

ggplot(father.son, aes(x = fheight, y = sheight)) + 
  geom_point(alpha = 0.5) + 
    xlab("Father's Height (in)") + ylab("Son's Height (in)") + labs(title = "Son's height by Father's Height") +
  geom_line(aes(x = line_x, y = line_y), data = sd_line, colour = 'blue')+ geom_hline(yintercept = m_y, colour = 'green') + theme_bw()
```

--- 

### Which line?
```{r, echo = F, message=F, warning=F}
ggplot(father.son, aes(x = fheight, y = sheight)) + 
  geom_point(alpha = 0.5) + 
    xlab("Father's Height (in)") + ylab("Son's Height (in)") + labs(title = "Son's height by Father's Height") +
  geom_line(aes(x = line_x, y = line_y), data = sd_line, colour = 'blue')+ geom_hline(yintercept = m_y, colour = 'green') + geom_smooth(method = 'lm', se = F, colour = 'red', fullrange = T) + theme_bw()
```

---

### Graph of Averages

```{r, echo = F, message = F, include = F}
father.son = as.data.table(father.son)
father.son[, bin := round(fheight)]
means = father.son[,list(m = mean(sheight),
                     count = length(fheight)),by=bin]
```

```{r, echo = F, message=F, warning=F}
ggplot(father.son, aes(x = fheight, y = sheight)) + 
  geom_point(alpha = 0.1) + 
    xlab("Father's Height (in)") + ylab("Son's Height (in)") + labs(title = "Son's height by Father's Height") +
  geom_line(aes(x = line_x, y = line_y), data = sd_line, colour = 'blue')+ geom_hline(yintercept = m_y, colour = 'green') + geom_smooth(method = 'lm', se = F, colour = 'red', fullrange = T) + 
  geom_point(aes(x = bin, y = m, size = count), data = means, alpha = 0.5) + theme_bw()
```

---

### Which line?

The **red** line above is the prediction using **least squares**.

It closely approximates the conditional mean of **son's height** ($Y$) across values of **father's height** ($X$).

How do we obtain this line mathematically?

We can do it the same way we obtained the mean!

---

### Minimizing the Distance

We are going to choose an intercept $a$ and slope $b$ such that:

$\hat{y}_i = a + b \cdot x_i$

and that minimizes the distance between the predicted ($\hat{y}_i$) and true ($y_i$) values:

$\sqrt{\sum\limits_i^n (y_i - \hat{y_i})^2}$

minimizes the length of  $\mathbf{e} = y_i - \hat{y_i}$

---

### Minimizing the Distance

Another way of thinking of this is in terms of **residuals**, or the difference between true and predicted values using the equation of the line. (Prediction errors)

$e_i = y_i - \hat{y_i}$

Minimizing the distance also means minimizing the sum of squared **residuals** $e_i$
  
- this is exactly what we did above with the mean

---

```{r, echo = F, message=F, warning=F}
m = lm(sheight~ fheight, father.son)
ols_residuals = data.frame(y= father.son$sheight, y_hat = m$fitted.values, x = father.son$fheight)

ggplot(father.son, aes(x = fheight, y = sheight)) + geom_point(alpha = 0.5) + 
   geom_point(alpha = 0.5) + 
    xlab("Father's Height (in)") + ylab("Son's Height (in)") + labs(title = "Son's height by Father's Height")   + geom_smooth(method = 'lm', se = F, colour = 'red', fullrange = T) + geom_segment(aes(x=x, xend=x, y=y_hat, yend=y), data = ols_residuals, col = 'darkred', alpha = 0.2) + theme_bw() + ggtitle("Residuals") 
  
```



---

### Minimizing the Distance

(Proof itself is not something you need to memorize)

We need to solve this equation: 

$$\min_{a,b} \sum\limits_i^n (y_i - a - b x_i)^2$$
Choose $a$ and $b$ to minimize this value, **given $x_i$ and $y_i$**

We can do this with calculus: solve for when first derivative is $0$

ILLUSTRATE

---

### Minimizing the Distance

First, we take derivative with respect to $a$: yields:

$-2 \left[ \sum\limits_i^n (y_i - a - b x_i) \right] = 0$

$\sum\limits_i^n y_i -  \sum\limits_i^n a - \sum\limits_i^n b x_i  = 0$

$-\sum\limits_i^n a   = -\sum\limits_i^n y_i + \sum\limits_i^n b x_i$

$\sum\limits_i^n a   = \sum\limits_i^n y_i - b x_i$

---

### Minimizing the Distance

$\sum\limits_i^n a = \sum\limits_i^n y_i - b x_i$

Dividing both sides by $n$, we get:

$a = \bar{y} - b\bar{x}$

Where $\bar{y}$ is mean of $y$ and $\bar{x}$ is mean of $x$.

**Implication**: regression line goes through the **point of averages** $\bar{y} = a + b \bar{x}$

---

### Minimizing the Distance

Next, we take derivative with respect to $b$: 

$-2 \left[ \sum\limits_i^n (y_i - a - b x_i) x_i\right] = 0$

$\sum\limits_i^n (y_i - (\bar{y} - b\bar{x}) - b x_i) x_i = 0$

$\sum\limits_i^n y_ix_i - \bar{y}x_i + b\bar{x}x_i - b x_ix_i= 0$


---

### Minimizing the Distance

$\sum\limits_i^n (y_i - \bar{y})x_i = b\sum\limits_i^n (x_i - \bar{x})x_i$

Dividing both sides by $n$ gives us:

$\frac{1}{n}\sum\limits_i^n y_ix_i - \bar{y}x_i = b\frac{1}{n}\sum\limits_i^n x_i^2 - \bar{x}x_i$

$\overline{yx} - \bar{y}\bar{x} = b \overline{xx} - \bar{x}\bar{x}$

$Cov(y,x) = b \cdot Var(x)$

$\frac{Cov(y,x)}{Var(x)} = b$

---

## Deriving Least Squares

### The slope:


$$b = \frac{Cov(x,y)}{Var(x)}$$

- Expresses how much mean of $Y$ changes for a 1-unit change in $X$
- When expressed as function of correlation coefficient $r$, we see this rise ($SD_y$) over the run ($SD_x$)


---

### The Intercept:


$$a = \overline{y} - \overline{x}\cdot b$$

Shows us that at $\bar{x}$, the line goes through $\bar{y}$. The regression line (of predicted values) goes through the point $(\bar{x}, \bar{y})$ or the point of averages.

---

### Deriving Least Squares

There are other ways to derive least squares.

- Because we eventually want multiple variables, need to build from an intuition rooted in matrices and their relationship to distance.
- Adding more variables means minimizing distance of $\hat{Y}$ to $y$ in a $p > 3$ dimensional space, so it gets weird.

---

### Key facts about regression:

The math of regression ensures that: 

$1$. **the mean of the residuals is always zero**. Because we included an intercept ($a$), and the regression line goes through the point of averages, the mean of the **residuals** is always 0. $\overline{e} = 0$. This is also true of residuals of the mean.

$2$. $Cov(X,e) = 0$.  This is true by definition of how we derived least squares. We will see why this is next week.

- Also means: Correlation of $X$ and residuals $e$ is exactly $0$. **Why is this?**

$3$. Must be that $Var(X) > 0$ in order to compute $a$ and $b$. **Why is this?**

---

### In small groups:

Using the `data_2` from [https://pastebin.com/LeSpqKKk](https://pastebin.com/LeSpqKKk)

Take $Y =$ `gop_change` and $X =$ `enlistment` (board)

1. Make functions to calculate $a$ and $b$; then calculate $a$ and $b$
2. Interpret $a$ in terms of CEF.
3. What is $\hat{y}$ when `enlistment` is 0.2? What does this mean in terms of the CEF?
2. Calculate $e$
3. Calculate the mean of $e$
4. Calculate the correlation of $X$ and $e$


# Summary

---

### Key ideas

These facts are mathematical truths about least squares. Unrelated to assumptions needed for statistical/causal inference.

- Can fit least squares (LS) to **any** scatterplot, if $X$ has positive variance.

- LS line minimizes the sum of squared residuals (minimizes the distance between predicted $\hat{Y}$ and actual values $y$).

- LS line always goes through point of averages; can be computed exactly from "graph of averages"

- Residuals $e$ are always uncorrelated with $x$ if there is an intercept. Will see why next week.

---

### Key ideas

- We have no addressed in any way how this relates to a statistical model or a causal model.
- **linear conditional expectation function** should be evaluated visually (does it make sense?)
- If the conditional expectation function is non-linear (e.g. a "U" shape in mean of $y$ across values of $x$) linear regression is **best linear approximation**.


