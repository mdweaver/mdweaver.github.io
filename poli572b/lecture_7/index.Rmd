---
title: "POLI 572B"
author: "Michael Weaver"
date: "March 1, 2021"
widgets: [bootstrap]
output: 
  revealjs::revealjs_presentation:
    theme: white
    highlight: haddock
---

```{r setup, include = F}
require(knitr)
require(magrittr)
require(kableExtra)
require(ggplot2)
require(grid)
require(data.table)
require(UsingR)
options("kableExtra.html.bsTable" = T)

acs_data = fread("acs_lawmed.csv")
acs_data[, FEMALE := SEX]
acs_data[, MARST_f := as.factor(MARST)]
set.seed(1234)
acs_data = acs_data[sample(1:nrow(acs_data), size = 10000)]
acs_data[, MALE := abs(FEMALE - 1)]
acs_data[, sex := as.factor(ifelse(FEMALE == 1, 'Female', 'Male'))]

brexit = fread("analysisData.csv")
```

<style type="text/css">
  .reveal h2,h3,h4,h5,h6 {
    text-align: left;
  }
  .reveal p {
    text-align: left;
  }
  .reveal ul {
    display: block;
  }
  .reveal ol {
    display: block;
  }
  .table-hover > tbody > tr:hover { 
  background-color: #696969;
  color: #FFFFFF;
  }
</style>

# Multivariate Least Squares

--- 

### Objectives

**Recap**

- Multivariate Least Squares mathematical properties


**Interpretation**

- Best Linear Approximation of CEF
- Dummy Variables
- Slopes and Intercepts

---

### Today

- algorithm with mathematical properties
    - what is the algorithm doing?
    - what are mathematical facts about the algorithm?
  
--- 

### Today

- How does the algorithm relate back to the conditional expectation function?

---

### Today

- Interpreting simple scenarios:

    - dummy variables
    - slopes and intercepts

# Recap

---

### Multivariate Least Squares:

Matrix equation fits **mean** of $Y$ as a **linear** function of many variables:

$$Y_i = b_0 + b_1 X_{1i} + b_2 X_{2i} + \ldots + b_k X_{ki}$$

$\hat{Y_i}$ is the predicted **mean** of $Y$ given values of $\mathbf{X}_i$

---

### Least Squares

We obtain coefficients $b_0, b_1, \dots, b_k = \beta$:

$$(\mathbf{X}'\mathbf{X})^{-1}\mathbf{X}'\mathbf{Y} = \mathbf{{\beta}}$$

This is the matrix formula for **least squares** regression. 

If $X$ is a column vector of $1$s, $\beta$ is just the mean of $Y$. (We just did this)

If $X$ is a column of $1$s and a column of $X_1$, it is an intercept and slope on $X$.


---

$$\mathbf{X} = \begin{pmatrix} 1 & x_1 \\ \vdots & \vdots \\ 1 & x_n \end{pmatrix}; \mathbf{Y} = \begin{pmatrix} y_1 \\ \vdots \\ y_n \end{pmatrix}$$

---

$$\mathbf{X}\beta = \begin{pmatrix} 1 & x_{11} & x_{21} \\ \vdots & \vdots & \vdots \\ 1 & x_{1n} & x_{2n} \end{pmatrix} \begin{pmatrix} b_0 \\ b_1 \\ b_2 \end{pmatrix} = \begin{pmatrix} \hat{y_1} \\ \vdots \\ \hat{y_n} \end{pmatrix} = \hat{Y}$$

---

### Mathematical Requirements:

1. Matrix $X$ has "full rank"
  - This means that all of the columns of $X$ are **linearly independent**.
    - cannot have two identical columns
    - cannot have  set of columns that sum up to another column multiplied by a scalar
  - If $X$ is not full rank, cannot be inverted, cannot do least squares.

2. $n \geq p$: we need to have more data points than variables in our equation
  - no longer trivial with multiple regression

---

### Key facts about regression:

The mathematical procedures we use in regression ensure that: 

$1$. **the mean of the residuals is always zero**. Because we included an intercept ($a$ or $b_0$), and the regression line goes through the point of averages, the mean of the **residuals** is always 0. $\overline{e} = 0$. This is also true of residuals of the mean.

---

### Key facts about regression:

The mathematical procedures we use in regression ensure that: 

$2$. $Cov(X,e) = 0$. This is true by definition of how we derived least squares. 

--- 

### Key facts about regression:

The mathematical procedures we use in regression ensure that: 

$3$. $\overline{Y} = \overline{X}'\beta$: the predicted value $\hat{Y}$ when all variables in $X$ are at their mean is the mean of $Y$.

--- 

### Key facts about regression:

The mathematical procedures we use in regression ensure that: 

$4$. Slope $b_k$ captures change in $Y$ due to variation in  $X_k$ that is **orthogonal**/**uncorrelated** to all other columns of $X$.

---

### Multivariate Least Squares:

$$Y = b_0 + b_1 X_1 + b_2 X_2 + \ldots + b_k X_k$$

$b_k = \frac{Cov(X_k^*, Y)}{Var(X_k^*)}$

where $X_k^* = X_k - \hat{X_k}$ obtained from the regression:

$X_{k} = c_0 + c_1 x_{1} + \ldots + c_{j} X_{j}$

$X_k^*$ is the residual from regressing $X_k$ on all other $X_{j \neq k}$

---

### Multivariate Least Squares:

How do we make sense of $b_k = \frac{Cov(X_k^*, Y)}{Var(X_k^*)}$ (if  $X_k^*$ as residual $X_k$ after regressing on all other $X_{j \neq k}$?)

- The slope $b_k$ is the change in $Y$ with a one-unit change in the part of $X_k$ that is **uncorrelated with**/**orthogonal to** the other variables in the regression.

# The CEF

--- 

### The CEF

$$E(Y_i | X_i) = f(X)$$

**Conditional**: mean of $Y$ is different at different levels of $X$

**Expectation**: expected value/mean of $Y$

**Function**: mathematical function links values of $X$ to mean of $Y$. Could be, does not need to be, **linear**


---

### Regression and CEF

How does multivariate regression relate back to Conditional Expectation Function?

- Angrist and Pischke give justification
- least squares is the **best linear approximation** of the conditional expectation function $E(Y_i | X_i)$


--- 

```{r, echo=F}
cef = acs_data[, list(INCEARN = mean(INCEARN), respondents = .N) , by = AGE]
setkey(cef, AGE)
ggplot(cef, aes(x = AGE, y = INCEARN, size = respondents)) + geom_point(alpha = 0.75) + theme_bw() + xlab("Age") + ylab("Earnings (USD)") + ggtitle("CEF of Earnings by Age for Lawyers and Doctors")

```


--- 

```{r, echo=F}
ggplot(cef, aes(x = AGE, y = INCEARN, size = respondents, weight = respondents)) + geom_point(alpha = 0.75) + theme_bw() + xlab("Age") + ylab("Earnings (USD)") + ggtitle("CEF of Earnings by Age for Lawyers and Doctors") + geom_smooth(method = 'lm', se = F)

```

---

### Least Squares and CEF

Least Squares predictions $\widehat{Y_i} = X_i'\beta$ gives the linear approximation of $E(Y_i | X_i)$ that has the smallest mean-squared error (minimum distance to the true $E(Y_i | X_i)$).

>- Least Squares could also fit a line for the mean of Y: $E(Y) | X$. Not just a prediction for each $Y_i$. **We could use $E(Y_i | X_i)$ as the dependent variable**

---

Exercise: Download the data. Contains mean income for each group of people with the same `AGE` in years

[https://www.dropbox.com/s/gyvv0mgvfic9jv7/acs_grouped.rda?dl=0](https://www.dropbox.com/s/gyvv0mgvfic9jv7/acs_grouped.rda?dl=0)

---

Exercise:

- Plot INCEARN by AGE

Use matrix calculations in `R` to obtain the linear approximation of $E(Y_i | X_i)$:

- What is the intercept? What does it mean?
- What is the slope? What does it mean?

---

```{r}
#With individual data...
X = cbind(1, acs_data$AGE)
y = acs_data$INCEARN

beta = solve(t(X) %*% X) %*% t(X) %*% y

beta
```

Do your estimates agree? Why or why not?

---

We are not accounting for the **number of people in each value of $X_i$**

```{r}
#With aggregate data...
X = cbind(1, cef$AGE)
y = cef$INCEARN
w = cef$respondents 
w = w * diag(length(w))
beta = solve(t(X) %*% w %*% X) %*% t(X) %*% w %*% y

beta
```

---

### Least Squares and CEF

We can reproduce any individual-level least squares by:

1. Collapsing data to all unique values $X_i$ to groups: $X_g$ across $g \in G$
2. Take the group mean of $Y_i$: $Y_g$
3. Regress $Y_g$ on $X_g$, weighting by group size $n_g$.

We are directly modelling the conditional expectation of $E(Y_i | X_i)$ as a linear function.


---

### Least Squares and CEF

Least Squares estimates the **mean of $Y$** as a function of values of  $X$.

- Conditional Expectation Function
- or $E(Y | X = X_i)$ is best prediction $\hat{Y_i} | X_i$

# Dummy Variables

--- 

### Dummy Variables

What are they?

- Binary variables taking $1$ if the observation has some attribute, $0$ otherwise.

- Can be used for Yes/No categorical variables
- Can be used for variables with **many** mutually exclusive categories (e.g. race, marital status, etc.)

How do we make them?

- Easy to do in `R` with `as.factor()`: each unique value will get a dummy.

```lm(y ~ as.factor(any_variable))```

--- 

### Dummy Variables

```{r}
m1 = lm(INCEARN ~ FEMALE, acs_data)
```

1. What does the design matrix $X$ in this regression look like?
2. What does the $b_0$ tell us?
3. What does the $b_1$ tell us?

---

### Dummy Variables

```{r}
m2 = lm(INCEARN ~ FEMALE + MALE, acs_data)
```

1. What does the design matrix $X$ in this regression look like?
2. What does the $b_0$ tell us?
3. What does the $b_1$ tell us?
4. What does the $b_2$ tell us?

---

### Dummy Variables

```{r}
m3 = lm(INCEARN ~ -1 + FEMALE + MALE, acs_data)
```

1. What does the design matrix $X$ in this regression look like?
2. What does the $b_0$ tell us?
3. What does the $b_1$ tell us?

--- 

```{r}
#Design matrix
model.matrix(m1) %>% head

#Interpret:
coefficients(m1)
```

--- 

```{r}
#Design matrix
model.matrix(m2) %>% head

#Interpret:
coefficients(m2)
```

--- 

```{r}
#Design matrix
model.matrix(m3) %>% head

#Interpret:
coefficients(m3)
```

---

### Dummy Variables

**What do they do?**

If there is an intercept

- **one category must be excluded** ("reference group"); otherwise linear dependence.  (`R` will do this by default)
- coefficient is **difference in means** between the group indicated by the dummy and the intercept ("reference group" mean)

If there is no intercept:

- all categories must be **included**
- fits separate **intercepts** for each group: coefficient is the mean of the group indicated by the dummy (when all other variables are $0$).

---

### Exercise:

How did different regions in the UK vote on Brexit?

- Download the `brexit` data: [https://www.dropbox.com/s/b5t5aao5frjpsw6/brexit.rda?dl=0](https://www.dropbox.com/s/b5t5aao5frjpsw6/brexit.rda?dl=0)
- use the `table` function to find out how what the different `Region` names are
- use the `hist` function to visually examine the distribution of the leave vote (`Pct_Lev`)

---

### Exercise:

Using the `lm` function:

1. Regress `Pct_Lev` on `Region`
2. Interpret the coefficients returned (`summary(your_model)`)
3. Regression `Pct_Lev` on `-1 + Region`
4. Interpret the coefficients.
5. Without looking at `your_model$fitted.values`, what can you say about the $\widehat{Y}$ produced by these two regressions?

---



### Example:

```{r}

table(brexit$Region)

```

---


```{r}

hist(brexit$Pct_Lev)

```

---

### Dummy Variables
```{r, echo = F, include = T, message = F}
levels = c('Scotland', 'London', 'North West', 'Yorkshire and The Humber', 'North East', 'West Midlands', 'East Midlands', 'South West', 'East', 'South East', 'Wales', 'Northern Ireland')
brexit[, RegionU := factor(Region, levels = levels)]

lm(Pct_Lev ~ RegionU, brexit) %>% summary
```

How is this different from what you produced?

---

### Dummy Variables

During the Brexit vote, many areas thought to support "Remain" experienced heavy rainfall. `total_precip_prepolling` is the number of mm of rain that fell just before polls were open. 

1. Regress `Pct_Lev` on `total_precip_prepolling + Region`.
2. Thinking about the least squares estimator: how is the slope on `total_precip_prepolling` on calculated in this case? (think of $X_k^*$)
3. In light of $2$, what does the slope on `total_precip_prepolling` mean?
  
---

```{r, echo = F, include = T, message = F}
lm(Pct_Lev ~ total_precip_prepolling + Region, brexit) %>% summary
```
  
---

### Exercise:

$2$: We regress Percent Leave on the **residual** pre-election precipitation. 

The other variable are **dummies for region.**  Thus, the $\widehat{precipitation_i}$ is the **mean** precipitation in each region.

So, residual $precipitation_i^*$, is difference between precipitation in a consituency and the mean precipitation in the region.

### Dummy Variables

You can change the "reference group" like this. This code shifts "Scotland" to the reference by making it the first "level" in the factor.

```
levels = c('Scotland', 'London', 'North West', 'Yorkshire and The Humber', 'North East', 'West Midlands', 'East Midlands', 'South West', 'East', 'South East', 'Wales', 'Northern Ireland')
brexit[, RegionU := factor(Region, levels = levels)]
```

# Interpretation

---

### Example

Let's say we want to get the **conditional expectation function** for yearly earnings of professionals as a function of hours worked, profession, gender, and age.

Using data from the American Community Survey, we can look at how hours worked is related to earnings for doctors and lawyers. 

- `UHRSWORK` (the usual hours worked per week)
- `FEMALE` (an indicator for female)
- `AGE` (in years)
- `LAW` (an indicator for being a lawyer)
- `INCEARN` (total annual income earned in dollars). 

---

### Example

Let's now find the (approximate) linear conditional expectation function of earnings, across hours worked, age, profession, and gender:

```{r}
ls = lm(INCEARN ~ UHRSWORK + AGE + LAW + FEMALE, acs_data)
```

---

### Example

```{r, echo = F}
summary(ls)
```

```{r, echo = F, include = F, message = F, warning=F}
residual_hrswork = lm(FEMALE ~ AGE + LAW + UHRSWORK, acs_data)$residuals
plot_data = rbind(acs_data[, list(INCEARN, AGE, LAW, FEMALE, UHRSWORK)], acs_data[, list(INCEARN, AGE, LAW, FEMALE, UHRSWORK)])
plot_data[, type := factor(rep(c('Observed', 'Residual'), each = nrow(acs_data)))]
plot_data[, FEMALE := as.numeric(FEMALE)]
plot_data[type == 'Residual', FEMALE := residual_hrswork]
```

How do we make sense of, e.g., the slope on `FEMALE`?

---

### Example

```{r, echo = F, warning=F, message=F}
ggplot(plot_data, aes(x = AGE, y= FEMALE)) + geom_point(alpha = 0.01) + geom_smooth(method = 'lm', colour = 'red', se = F) +  geom_smooth(method = 'loess', colour = 'blue', se = F) + theme_bw() + facet_wrap(~type) + ggtitle("Gender Residual on Age")
```

---

### Example

```{r, echo = F, warning=F, message=F}
ggplot(plot_data, aes(x = LAW, y= FEMALE)) + geom_point(alpha = 0.01)  + geom_smooth(method = 'lm', colour = 'red', se = F) + geom_smooth(method = 'loess', colour = 'blue', se = F) + theme_bw() + facet_wrap(~type) + ggtitle("Gender Residual on Profession")
```

---

### Example

```{r, echo = F, warning=F, message=F}
ggplot(plot_data, aes(x = UHRSWORK, y= FEMALE)) + geom_point(alpha = 0.01) + geom_smooth(method = 'lm', colour = 'red', se = F) + geom_smooth(method = 'loess', colour = 'blue', se = F) + theme_bw() + facet_wrap(~type)  + ggtitle("Gender Residual on Hours Worked")
```

---

### Example

```{r, echo = F, warning=F, message=F}
ggplot(plot_data, aes(x = FEMALE, y = INCEARN)) + geom_point(alpha = 0.01) + geom_smooth(method = 'lm', colour = 'red', se = F) + theme_bw() + facet_wrap(~type)  + ggtitle("Earnings on Residual Gender")
```

---

### Exercise:

```{r, echo=F}
ggplot(cef, aes(x = AGE, y = INCEARN, size = respondents, weight = respondents)) + geom_point(alpha = 0.75) + theme_bw() + xlab("Age") + ylab("Earnings (USD)") + ggtitle("CEF of Earnings by Age for Lawyers and Doctors") + geom_smooth(method = 'lm', se = F)

```

---

### Example

Previous slide shows linear approximation of CEF of earnings by age.

In your small groups: 

1. Can you think of a way to use least squares to **better** approximate the true conditional expectation function of earnings by Age?

2. How would you do it?

3. Try it out

4. How would you incorporate this into the example above? `ls = lm(INCEARN ~ UHRSWORK + AGE + LAW + FEMALE, acs_data)`?
