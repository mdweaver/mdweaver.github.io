---
title: "POLI 110: Confounding"
author: "Michael Weaver"
date: "March 22, 2023"
output: ioslides_presentation
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
require(haven)
require(data.table)
require(ggplot2)
require(magrittr)
require(ggdag)
```

# Solutions to Confounding

## Outline

1. Confounding
2. Solutions to Confounding
  - Experiments
    - Assumptions
    - Limitations
  - Internal vs External Validity
    

# Confounding


## Confounding a Problem

- FPCI $\to$ we can't know whether some $X$ causes $Y$ to change for any individual case.

- Correlation as a solution is prone to a **bias**: confounding.

We improve correlation and plausibly solve confounding, **if we make some assumptions**, and we can **defend those assumptions**.


# Example

---

<img src="./fakenews.png" width=100%>


## Pandemic Misinformation

Story above not false, but misleading:

- most widely shared story about the vaccine in 2021-2022
- $.0018\%$ of US vaccine recipients have died
- apprx. 8000 people die in the US die each day for other reasons.


## Pandemic Misinformation

Blatantly false or misleading information about COVID-19 and COVID vaccine circulate widely on social media [See here.](https://www.washingtonpost.com/technology/2020/12/18/faq-coronavirus-vaccine-misinformation/)

Even [public officials circulate misinformation.](https://www.ctvnews.ca/health/canadian-cardiologists-debunk-florida-covid-19-vaccine-claims-1.6312178)

Misleading information [can affect behavior](https://www.nber.org/system/files/working_papers/w27417/revisions/w27417.rev0.pdf), including reducing willingness to be vaccinated. 

## Pandemic Misinformation

What are the effects of pandemic misinformation?

- Does receiving misinformation make people less likely to be vaccinated?

## Pandemic Misinformation

What if we look at Facebook users:

- use Facebook history to see if they were exposed to vaccine misinformation on their feed
- survey them and ask whether they have they been vaccinated

>- Why might there be confounding here?


## A similar problem {.build}

**Does knowing transgender people increase support for funding gender-affirming care?**

### Think...

In the recent AMS election, you were asked vote on whether to increase student fees by \$8 to cover gender-affirming care.

If you had a friend on campus who was trans/non-binary, would you have voted "yes" or "no" on this measure?

If you did not have a friend on campus who was trans/non-binary, would you have voted "yes" or "no" on this measure?

Go to [menti.com](https://www.menti.com) and enter $4434 \ 9124$ (there are THREE questions, click through to answer all of them)


## Confounding a risk

```{r, echo = F, message=F, warning=F}
dagify(
       #flag ~ assign,
       #block ~  assign,
       fake ~ distrust,
       vaccine ~ fake + distrust,
       exposure = "fake", 
       outcome = 'vaccine',
       labels = c("fake" = "Consume\nMisinformation",
                  "distrust" = "Distrust\nExperts",
                  #"flag" = "Flag News\nas 'Fake'",
                  #'filter' = "Reveal\nCensoring",
                  "vaccine" = "COVID\nVaccination"
                  #"exp" = "Other\nStuff"
                  )
                  )  %>%
  tidy_dagitty(layout='auto') %>%
ggplot(aes(x = x, y = y, xend = xend, yend = yend)) +
  geom_dag_edges_link() +
  geom_dag_text(mapping = aes(label = label), colour = 'black') +
  theme_dag() +
  scale_adjusted()
```

## Confounding a risk

```{r, echo = F, message=F, warning=F}
dagify(
       #flag ~ assign,
       #block ~  assign,
       fake ~ distrust,
       vaccine ~ fake + distrust,
       exposure = "fake", 
       outcome = 'vaccine',
       labels = c("fake" = "Trans\nFriends",
                  "distrust" = "'Traditional'\nValues",
                  #"flag" = "Flag News\nas 'Fake'",
                  #'filter' = "Reveal\nCensoring",
                  "vaccine" = "Support\nGender Care"
                  #"other" = "Other\nStuff"
                  )
                  )  %>%
  tidy_dagitty(layout='auto') %>%
ggplot(aes(x = x, y = y, xend = xend, yend = yend)) +
  geom_dag_edges_link() +
  geom_dag_text(mapping = aes(label = label), colour = 'black') +
  theme_dag() +
  scale_adjusted()
```

---

What if we could do this:

```{r, echo = F, message=F, warning=F}
dagify(
       #flag ~ assign,
       #block ~  assign,
       fake ~ other,
       vaccine ~ fake + distrust,
       exposure = "fake", 
       outcome = 'vaccine',
       labels = c("fake" = "Consume\nMisinformation",
                  "distrust" = "Distrust\nExperts",
                  #"flag" = "Flag News\nas 'Fake'",
                  #'filter' = "Reveal\nCensoring",
                  "vaccine" = "COVID\nVaccination",
                  "other" = "?"
                  )
                  )  %>%
  tidy_dagitty(layout='nicely') %>%
ggplot(aes(x = x, y = y, xend = xend, yend = yend)) +
  geom_dag_edges_link() +
  geom_dag_text(mapping = aes(label = label), colour = 'black') +
  theme_dag() +
  scale_adjusted()
```


## Pandemic Misinformation

What if Facebook conducted a test of a new algorithm that classified pandemic misinformation in Facebook posts and shared links?

- for a **random** subset of users, this algorithm was used to **remove** misinformation from their Feed
- for a **random** subset of users, the algorithm was not used (so misinformation remained on their Feed)

>- This random assignment of the misinformation filtering **affects** misinformation, may not (?) affect vaccination through another path. 
>- **Randomness** of misinformation means no other variables affect it

## Experiments

**FPCI:** We cannot know the causal effect of $X$ on $Y$ for a specific case.

**Correlation** of $X$ and $Y$ for different cases may suffer from confounding

### **Experiments**  are one solution

Under certain assumptions, we can treat correlation as an **estimate** (an **inference** about) the **average** causal effect of $X$ on $Y$. 

- We can't know the causal effect for individual cases, but can make **unbiased** inferences about the **average effect** across all cases

## **Experiments**

Experiments give us **unbiased** (no confounding) relationship between $X$ and $Y$, with two key **assumptions**:

1. **Random Assignment** to "Treatment" and "Control": all cases have equal probability of ending up in each "condition" (exposure to $X$)
2. **Exclusion Restriction** (only one thing is changing: $X$)

<br>
<small>$^*$Technically, there are other assumptions, but not important for this class</small>

---

### **Randomization** solves Confounding

- Randomization ensures that cases in treatment and control have similar potential outcomes
- Randomization balances cases with similar values of confounding variable $W$ in treatment and control (breaks the link $W \to X$)

Cases are **about the same** in terms of potential outcomes, on average:

  - average in control group is **observable** "counterfactual" for average in treatment group
  - average in treatment group is **observable** "counterfactual" for average in control group
  - EXACTLY like with random sampling: [https://mdweaver.shinyapps.io/shiny/](https://mdweaver.shinyapps.io/shiny/)


---

### **Exclusion Restriction**  

assumption means we assume we aren't *adding* confounding in the design of the experiment

If in the COVID misinformation experiment, "Treatment" group

- had COVID misinformation blocked from their Facebook Feed ($X$)
- the blocked posts were obscured with a warning that the content was misinformation ($W$)
- flagging content revealed that Facebook is censoring ($V$)

Multiple differences between "treatment" and "control"

-  Does vaccination change because of (a) removal of misinformation or (b) the warning that content was flagged or (c) revealing that Facebook censors?

## Exclusion Restriction

```{r, echo = F}
dagify(
       flag ~ assign,
       block ~  assign,
       filter ~ assign,
       vaccine ~ block + flag + filter + other,
       exposure = "assign", 
       outcome = 'vaccine',
       labels = c("assign" = "Random\nAssignment",
                  "block" = "Block\nFake News",
                  "flag" = "Flag News\nas 'Fake'",
                  'filter' = "Reveal\nCensoring",
                  "vaccine" = "COVID\nVaccination",
                  "other" = "Other\nStuff"
                  )
                  )  %>%
  tidy_dagitty(layout='auto') %>%
ggplot(aes(x = x, y = y, xend = xend, yend = yend)) +
  geom_dag_edges_link() +
  geom_dag_text(mapping = aes(label = label), colour = 'black') +
  theme_dag() +
  scale_adjusted()
```
  
  
## Experiments

Experiments are the best solution to confounding/FPCI

- If we can manipulate $X$ **at random**, we can find the true **average** causal effect of $X$ on $Y$ (no confounding)

But experiments have their limits:

- We can't always use them
- We make a trade-off by using experiments


# Experiments

## Example

### My 6-y/o son just received his second dose of his COVID vaccination

### But, we know about the fundamental problem of causal inference and about confounding...

>- How do we know that this vaccine actually **reduces** risk of infection?
>- How do we know that the vaccine **does not cause** adverse reactions?

## Vaccine Efficacy

BC CDC reports, e.g. hospitalization rates between those who are vaccinated vs. unvaccinated

- correlation between vaccination and COVID hospitalization

What could confound this correlation?

## Vaccine Clinical Trials

<img src="./vaccine_1.png" width=80%>

## Vaccine Clinical Trials

Clinical trials are **experiments**. Correlation between treatment and health outcomes are **causal** (do not have confounding) **assuming** that...

- assignment to treatment is **random**
- the **only difference** between treatment and control is the actual content of the vaccine.

## Random Assignment

- Blocks confounding
- Balances potential outcomes

TO THE BOARD

## Exclusion Restriction

```{r, echo = F}
dagify(
       shot ~ assign,
       vaccine ~  assign,
       careworker ~ assign,
       covid ~ shot + vaccine + careworker + other,
       exposure = "vaccine", 
       outcome = 'covid',
       labels = c("assign" = "Random\nAssignment",
                  "vaccine" = "Receive\nVaccine",
                  "shot" = "Receive\nShot",
                  'careworker' = "Careworker\nBehavior",
                  "covid" = "Get COVID",
                  "other" = "Other\nStuff"
                  )
                  )  %>%
  tidy_dagitty(layout='auto') %>%
ggplot(aes(x = x, y = y, xend = xend, yend = yend)) +
  geom_dag_edges_link() +
  geom_dag_text(mapping = aes(label = label), colour = 'black') +
  theme_dag() +
  scale_adjusted()
```

## Exclusion Restriction

Vaccine clinical trials...

- use placebos (control group receives a shot with no vaccine)
- "blind" treatment recipients to whether they receive vaccine or placebo
- "blind" care providers to whether they are injecting real vaccine or not

These **design** features of the experiment ensure that **only difference** between "assigned to treatment" and "assigned to control" is the vaccine.

## Vaccine Clinical Trials

<img src="./vaccine_2.png" width=100%>

## Vaccine Clinical Trials

We can know whether vaccination caused reduction in:

- COVID cases
- COVID hospitalization

or caused increase in:

- other health problems/adverse side effects

## Experiments

Might appear to be the only valid solution:

- if we don't know how cases would behave counterfactually (always the case)
- if we don't know what other causal factors affect X and Y

Experiments, under easy-to-check assumptions, let us find an unbiased causal relationship between $X$ and $Y$ using correlation

- Experiments likely have "internal validity"

## Internal Validity

**Internal Validity**

A **research design** (choice of which cases to compare using correlation) has internal validity when the correlation of $X$ and $Y$ it finds is the true causal effect of $X$ on $Y$ / does not suffer from confounding.

- studies with strong internal validity imply that we have very good reason to believe that the correlation of $X$ and $Y$ we observe actually implies the causal effect of $X$ on $Y$.
- **because** we can easily believe/check the assumptions (e.g. randomization)


## Vaccines in the "real world"

<img src="./vaccine_4.jpg" width=40%>

**Does the efficacy of vaccines in clinical trials translate to real world use??**

[https://www.menti.com/bz4gsu58z2](https://www.menti.com/bz4gsu58z2) or [menti.com](menti.com) $77 \ 00 \ 76$ (click through both questions)

>- How might people who enroll in clinical trials **be different**.
>- How might people **behave differently** in vaccine trials?

---

<div style='position: relative; padding-bottom: 56.25%; padding-top: 35px; height: 0; overflow: hidden;'><iframe sandbox='allow-scripts allow-same-origin allow-presentation' allowfullscreen='true' allowtransparency='true' frameborder='0' height='315' src='https://www.mentimeter.com/embed/81f7c4eecbaf6d3621a103240df0dab1/1ba7d6577397' style='position: absolute; top: 0; left: 0; width: 100%; height: 100%;' width='420'></iframe></div>


## Experiments: Limitations

**What can we manipulate?**

- economic growth? democracy? violence? hate speech?

**Who/what cases can we study?**

- who participates in psych labs? 
- is it ethical to experiment on people in developing countries?

>- Experiments have limited **external validity**

---

**External Validity**

is the degree to which the *causal relationship* we find *in a study* is relevant to the causal relationship in our causal question/claim

- Study has external validity if the relationship found is true for **the cases we are interested in**

    - is study has sampling bias (sample in study different from population of interest), may lack external validity
    - E.g. Transgender canvassing

- Study has external validity if the causal variable in the study **maps onto the concept/definition of the cause** in the causal claim.

    - E.g. Fox News media effects vs. Lab studies of media effects
    - example: hate speech

## Always a Trade-off:

More internal validity (unbiased estimate of causal effect) comes at the cost of external validity (relevance of study sample or cause to the theory)

- easier to experimentally manipulate **minor** causal forces for **small groups** of people/cases 
- hard/unethical to experimentally manipulate **important causal factors** in society **more broadly**


## Conclusion

- Experiments solve confounding:
  - assuming: Random Assignment
  - assuming: Exclusion Restriction
- Solutions to confounding:
  - require making assumptions
  - trade off between internal validity and external validity 
- Next week:
  - conditioning as a solution to confounding