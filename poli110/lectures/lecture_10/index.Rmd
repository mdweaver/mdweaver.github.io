---
title: "Investigating Politics"
author: "Michael Weaver"
date: "March 5, 2018"
output: ioslides_presentation
df_print: paged
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
require(magrittr)
require(kableExtra)
require(knitr)
require(ggplot2)
require(gtools)
```
<style>
.forceBreak { -webkit-column-break-after: always; break-after: column; }
h3, h4 {font-weight: bold;
        color: #515151;}
</style>

# Testing Causal Theories

## Recap {.build}

### Causal Theory

- Concepts $\xrightarrow{}$ Variables
    - Independent (Cause) and Dependent (Outcome) Variables
- Direction of effect
- Causal logic

## Testing

### Based on causal theory:

**hypotheses**: a statement about what *we should expect to observe* if a causal claim is true. (Also could call this *empirical prediction*)


## Testing {.build}

Causal theory may generate several hypotheses:

- based on main causal claim
- based on steps in causal logic

### What are hypotheses for causal claims?

Presence of cause and presence of effect. Absence of cause and absence of effect.

Higher/lower levels of X (cause) appear Higher/lower levels of Y (outcome).

# Examples

## Economic growth and democracy

Claim:

#### Higher rates of economic growth cause democratization to be more likely

## Economic growth and democracy

<img src="./growth_democracy.png" width=100%>

## Firearms and crime

Claim:

#### Higher rates of firearms ownership cause crime to be less likely

## Firearms and crime

<img src="./guns_2.png" width=100%>

## Economic growth and Civil War

Claim: 

#### Lower rates of economic growth cause civil war to be more likely

## Economic growth and Civil War

<img src="./civilwar_growth.png" width=100%>

## Nick Cage and Drownings:

Claim:

#### Higher rates of Nick Cage films cause a higher rate of drowning deaths? 

## Nick Cage and Drownings:

<img src="./nick_cage.svg" width=100%>

## Not enough

Maybe we've been lead astray.

## Counterfactual Causality

### Claim:

X causes Y.

### Implies:

If X does not happen then Y would not happen

## Our examples as counterfactuals:

1. If economy had grown less (more), countries would have been less (more) democratic
2. If there were more (fewer) guns, there would be less (more) crime.
3. If there were more (less) economic growth, civil would be less (more) likely
4. If Nick Cage were in fewer (more) films, fewer (more) people would die by drowning

## Thinking Counterfactually

### Claim

X causes Y

### Implies

If claim is true, then in **one case** with some level of X and some level of Y:

If that case had a **different** level of X, then the level of Y would have been **different**.

## How do we observe this?

1. We can't make the same country have less economic growth and see if it doesn't democratize
2. We can't make the US gun ownership rate not change in the past and see if crime stays high
3. We can't make the same country have more economic growth to see if it prevents civil war
4. We can't unmake (or unsee) Ghost Rider 2 to see if fewer people would have died in 2012

## Fundamental Problem of Causal Inference

We never can observe a case under the counterfactual condition: we can only observe a case under one (the factual) condition.

## Let's see this in action:

We want to test whether $X$ causes higher $Y$.

For sake of simplicity, imagine a situation with $5$ cases. 

Each observation is indexed by $i$.

$X_i:$ value of cause $X$ for case $i$. $1$ for cause is present, $0$ for cause is absent

$Y_i:$ The outcome (dependent variable) for $i$

## What we can see

| i | $Y_i$ | $X_i$ |
|---|---------|--------|
| 1 | 6 | 1 |
| 2 | 2 | 0 | 
| 3 | 8| 1 | 
| 4 | 4 | 0 |
| 5 | 6 | 1 |

## Evidence for the claim? {.build}

Take average of $Y$ for $X = 1$ and for $X = 0$ and take the difference:

(1) $$\frac{6 + 8 + 6}{3} - \frac{2 + 4}{2}$$

(2) $$ \frac{20}{3} - \frac{6}{2}$$

(3) $$ \frac{20}{3} - \frac{9}{3} = \frac{11}{3}$$

### Having $X$ seems to mean higher $Y$

## Let's not get too excited

Fundamental Problem of Causal Inference:

- We don't know what cases would have looked like if they had the **other** value of $X$.
- That is: we don't know what they would look like under the counterfactual.

## We can imagine though

Counterfactual causality means that every case has a value for the outcome that it **would take** under the factual and counterfactual conditions. We call these **potential outcomes**

## We can imagine though

We want to test whether $X$ causes higher $Y$.

For sake of simplicity, imagine a situation with $5$ cases. 

Each observation is indexed by $i$.

$X_i:$ value of cause $X$ for case $i$. $1$ for cause is present, $0$ for cause is absent

$Y_i^0:$ The outcome (dependent variable) for $i$ when $X_i = 0$

$Y_i^1:$ The outcome (dependent variable) for $i$ when $X_i = 1$

## If we saw the potential outcomes:

| i | $Y_i^0$ | $Y_i^1$ | $Y_i^1 - Y_i^0$ |
|---|---------|--------|-----------------|
| 1 | 6 | 6 | 0 |
| 2 | 2 | 2 | 0 |
| 3 | 8 | 8 | 0 |
| 4 | 4 | 4 | 0 |
| 5 | 6 | 6 | 0 |

## Did our test work?

### Our test: compare $Y$ for $X = 1$ and $X = 0$

We found that $Y$ was $\frac{11}{3}$ higher when $X = 1$

### Truth: $Y_i^1 - Y_i^0$ 

The truth is the average difference for each case between condtion with $X$ and condition without $X$

Truth is that there is **no difference**

## But can only see *factual* outcome

**fundamental problem of causal inference**:

| i | $Y_i^0$ | $Y_i^1$ | $Y_i^1 - Y_i^0$ |
|---|---------|--------|-----------------|
| 1 | ? | 6 | ? |
| 2 | 2 | ? | ? |
| 3 | ?| 8 | ? |
| 4 | 4 | ? | ? |
| 5 | ? | 6 | ? |

## What good is this?

- We can't trust comparing higher/lower levels of X and Y
- We can't observe cases in counterfactual condition

So, we still have fundamental problem of causal inference.

Can we learn **anything** about causality?

## Causal claims and Hypotheses {.build}

### We started with this:

1. Higher/lower levels of X (cause) appear with Higher/lower levels of Y (outcome).

### But we need something else:

2. Comparison between cases that are very similar (next best thing to seeing case under both cause, absence of cause)

### Why?

if causes deterministic: cases that have exact same set of causes acting on them behave exactly the same. 

## Mill's Method of Difference

Claim: $X$ is cause of $Y$

$A,B,C$ are other possible causes of $Y$.

|  | **Case 1** | **Case 2** |
|-------|------------|------------|
| **X** | 1 | 0 |
| **A** | **1** | **1** |
| **B** | **0** | **0** |
| **C** | **1** | **1** |
| **Y** | 1 | 0 |

## Mill's Method of Difference

|  | **Case 1** | **Case 2** |
|-------|------------|------------|
| **X** | **1** | **0** |
| **A** | 1 | 1 |
| **B** | 0 | 0 |
| **C** | 1 | 1 |
| **Y** | **1** | **0** |

## Mill's Method of Difference

### Problem!

|  | **Case 1** | **Case 2** |
|-------|------------|------------|
| **X** | **1** | **0** |
| **A** | 1 | 1 |
| **B** | 0 | 0 |
| **C** | 1 | 1 |
| **Y** | **1** | **1** |

## Mill's Method of Difference

### Problem!

|  | **Case 1** | **Case 2** |
|-------|------------|------------|
| **X** | **1** | **0** |
| **A** | 1 | 1 |
| **B** | **1** | **0** |
| **C** | 1 | 1 |
| **Y** | 1 | 0 |


## Comparative Method:

If causal claim that $X \rightarrow Y$, then:

We generate **empirical prediction**:

> If we observe **two cases** to be the same in all relevant respects except for value of $X$, then we should observe that the two cases differ in the value of $Y$

This **empirical prediction** based on a causal claim is called **comparative method** or **method of difference** (via John Stuart Mill)

## Difficulty:

What are the **relevant similarities**?  How many things need to be the same? How do we identify them?

## Difficulty:

1. Society is complex, humans and our institutions have lots of possible attributes, exposure to lots of different causes
2. We might not know all possible causes
3. Very difficult to actually measure all of these causes
4. Even if we did, might not find case that is exactly the same

## Worst Case Scenario:

### Claim:

$X \rightarrow Y$

but also the case that $\lbrace W_1, W_2, \ldots, W_\infty \rbrace \rightarrow Y$

### if true

1. We can't observe all $W_1, W_2, \ldots, W_\infty$
2. Unlikely to find cases that are the same on all $\mathbf{W}$

## What do we do? {.build}

We've been trying to find a factual case $j$ that can be a counterfactual for the factual case $i$: identical on all attributes except the cause $X$. 

### But, we've had a devil of a time doing it.

Why? No reason to assume any case is exactly like another. 

## What do we do? {.build}

Maybe we've been doing this wrong.

Instead of finding counterfactual for each case $i$...

### ... what if we looked for counterfactuals *on average*?

## Solving the FPCI

The fundamental problem of causal inference states we can't observe both $Y_i^1$ (outcome when exposed to cause) and and $Y_i^0$ (outcome when not exposed to cause) for a given $i$. 

But we can get around this if we look at many cases $i \in \lbrace 1 \ldots n \rbrace$ and take the average. 

## How?

We cannot know each individual causal effect: $\tau_i = Y_i^1 - Y_i^0$

But the **average causal effect** is the average of all individual causal effects:

$ACE = \frac{1}{n}\sum\limits_i^n{\tau_i}$

So it is also equal to this:

$ACE = \frac{1}{n}\sum\limits_i^n (Y_i^1 - Y_i^0)$

## How?

Which means it can be the difference between the averages of both potential outcomes:

$ACE = \Big( \frac{1}{n}\sum\limits_i^n Y_i^1 \Big) - \Big( \frac{1}{n}\sum\limits_i^n Y_i^0 \Big)$

## How?

But does this help us?

$ACE = \Big( \frac{1}{n}\sum\limits_i^n Y_i^1 \Big) - \Big( \frac{1}{n}\sum\limits_i^n Y_i^0 \Big)$

- $\frac{1}{n}\sum\limits_i^n Y_i^1$ is the average over all cases of the potential outcome when the cause is present.
- $\frac{1}{n}\sum\limits_i^n Y_i^0$ is the average over all cases of the potential outcome when the cause is absent.
- The FPIC means that for any case, we only observe $Y_i^1$ or $Y_i^0$, not both.

## How?

At best we can observe:

1. $\frac{1}{n}\sum\limits_i^n (Y_i^1 | X_i = 1)$: The average of $Y$ when $X$ is present for the cases where $X$ is actually present
2. $\frac{1}{n}\sum\limits_i^n (Y_i^0 | X_i = 0)$: The average of $Y$ when $X$ is absent for the cases where $X$ is actually absent

And we know from above that this can go wrong:

## 

| i | $Y_i^0$ | $Y_i^1$ | $Y_i^1 - Y_i^0$ |
|---|---------|--------|-----------------|
| 1 | ? | 6 | ? |
| 2 | 2 | ? | ? |
| 3 | ?| 8 | ? |
| 4 | 4 | ? | ? |
| 5 | ? | 6 | ? |

##

Take average for $X = 1$ and for $X = 0$ and take the difference:

(1) $$ \frac{6 + 8 + 6}{3} - \frac{2 + 4}{2}$$

(2) $$ \frac{20}{3} - \frac{6}{2}$$

(3) $$ \frac{20}{3} - \frac{9}{3} = \frac{11}{3}$$

## 

True effect is $0$ !

| i | $Y_i^0$ | $Y_i^1$ | $Y_i^1 - Y_i^0$ |
|---|---------|--------|-----------------|
| 1 | 6 | 6 | 0 |
| 2 | 2 | 2 | 0 |
| 3 | 8 | 8 | 0 |
| 4 | 4 | 4 | 0 |
| 5 | 6 | 6 | 0 |

## What do we need?

1. If the average outcome for cases with cause $X$ were similar to what the average outcome **would be** in the presence of $X$ for cases **without** $X$.
2. If the average outcome for cases without $X$ were similar to what the average outcome **would be** in the absence of $X$ for cases **with** $X$ 

Then: 

- $\frac{1}{n}\sum\limits_i^n (Y_i^1|X_i = 1) = \frac{1}{n}\sum\limits_i^n (Y_i^1)$
- $\frac{1}{n}\sum\limits_i^n (Y_i^0|X_i = 0) = \frac{1}{n}\sum\limits_i^n (Y_i^0)$

## What do we need?

### That'd be great! 

But we don't **know** the counterfactual outcomes, so how can we ensure they are similar?

## How?

Wait... 

### Didn't we just see a problem like this?

- We had a **population** we want to describe, but it was too large to observe every case
- We wanted a **sample** that looked like the population

### Random Sampling

A procedure that let us get a sample such that:

- Mean of sample is unbiased estimate of mean of population

## What do we need? {.build}

### If we *randomly* sampled

Some cases to get $X$ and some cases to not get $X$

#### Then...

1. The average $Y^1$ for cases with $X$ would be a sample of the average counterfactual $Y^1$ for cases without $X$
2. The average $Y^0$ for cases without $X$ would be a sample of the average counterfactual $Y^0$ for cases with $X$.

#### AND

$ACE = \frac{1}{n}\sum\limits_i^n (Y_i^1 | X_i = 1) - \frac{1}{n}\sum\limits_i^n (Y_i^0 | X_i = 0)$

All of which **is observable** because it is **factual**

## What is this miraculous procedure? {.build}

If you haven't guessed it:

This is the precise logic of a **randomized experiment**.

### It works because:

All cases have equal chance of being exposed to cause.

## Recap:

### Testing causal theories

We need to generate **emprirical predictions** or **hypotheses** about what we will observe if the causal theory is correct

## Recap:

### Testing causal theories

Causality implies a counterfactual.

Empirical Predictions for causal theories are that:

1. When we change the level of the independent variable (cause) for a specific case
2. The change in the level of the independent variable results in change in dependent variable (outcome)


## Recap:

### Testing causal theories

But we can't change the level of the independent variable for a case:

**Fundamental Problem of Causal Inference**: key part empirical prediction about causal theory is **never observable**

## Recap: {.build}

### Testing causal theories

**Fundamental Problem of Causal Inference** is solvable when 

1. we look at groups of cases in aggregate
2. we randomly sample cases to get exposed to the cause/not

### An experiment

## How do experiments work?

### Randomly assigning exposure to cause

"Treatment" and "Control" group have same potential outcomes

**because** they are two random samples from the same population

- Treatment group factual outcomes are same as counterfactual outcomes for Control
- Control group factual outcomes are same as counterfactual outcomes for Treatment


## How do experiments work?

Best way to understand is to do it!

# Take out writing tools

## Let's work through it

We have this table of potential outcomes:

| i | $Y_i^0$ | $Y_i^1$ | $Y_i^1 - Y_i^0$ |
|---|---------|--------|-----------------|
| 1 | 5 | 9 | 4 |
| 2 | 4 | 8 | 4 |
| 3 | 3 | 7 | 4 |
| 4 | 2 | 6 | 4 |

## Let's work through it

Could be: effect of campaign ad on campaign contributions:


| i | (\$) without ad | (\$) with ad | (\$) with - (\$) without |
|---|---------|--------|-----------------|
| 1 | 5 | 9 | 4 |
| 2 | 4 | 8 | 4 |
| 3 | 3 | 7 | 4 |
| 4 | 2 | 6 | 4 |



## Let's work through it

What the true average causal effect?

| i | $Y_i^0$ | $Y_i^1$ | $Y_i^1 - Y_i^0$ |
|---|---------|--------|-----------------|
| 1 | 5 | 9 | 4 |
| 2 | 4 | 8 | 4 |
| 3 | 3 | 7 | 4 |
| 4 | 2 | 6 | 4 |


## Let's work through it


$$\frac{4 + 4 + 4 + 4}{4} = 4$$


## Let's work through it

Let's run a randomized experiment:

- We will put 2 people in treatment (see the campaign ad)
- We will put 2 people in control (don't see the ad)

When we do this at random:

- we are choosing one possible partition of the sample 
- there are several possible partitions into Treatment/Control
- each possible partition is equally likely

## Let's work through it

Write down all possible treatment groups of size $2$ (e.g. (1,2)) 

| i | $Y_i^0$ | $Y_i^1$ | $Y_i^1 - Y_i^0$ |
|---|---------|--------|-----------------|
| 1 | 5 | 9 | 4 |
| 2 | 4 | 8 | 4 |
| 3 | 3 | 7 | 4 |
| 4 | 2 | 6 | 4 |

## Let's work through it

All possible treatment groups

```{r, echo = F}
assign = t(combn(1:4, 2))

kable(as.data.frame(assign))

```

## Let's work through it

Calculate the average $Y^1$ (\$ with ad) for every possible treatment (ad) group

| i | $Y_i^0$ | $Y_i^1$ | $Y_i^1 - Y_i^0$ |
|---|---------|--------|-----------------|
| 1 | 5 | **9** | 4 |
| 2 | 4 | **8** | 4 |
| 3 | 3 | **7** | 4 |
| 4 | 2 | **6** | 4 |

## Let's work through it

Calculate the average $Y^0$ (\$ with ad) for every corresponding control (no ad) group

| i | $Y_i^0$ | $Y_i^1$ | $Y_i^1 - Y_i^0$ |
|---|---------|--------|-----------------|
| 1 | **5** | 9 | 4 |
| 2 | **4** | 8 | 4 |
| 3 | **3** | 7 | 4 |
| 4 | **2** | 6 | 4 |

## Let's work through it


```{r, echo = F}

po_table = data.frame(i = 1:4, y0 = 5:2, y1 = 9:6)
assign = t(combn(1:4, 2))

effects = data.frame(T_Group = paste(assign[,1], assign[2], sep = ","),
           Y_1 = apply(assign, 1, function(x) mean(po_table$y1[x]) ),
           Y_0 = apply(assign, 1, function(x) mean(po_table$y0[-x]) )
           )

kable(effects)
```

## Let's work through it

Calculate the average causal effect for every possible experiment ((\$) ad - (\$) no ad) or $Y^1 - Y^0$

```{r, echo = F}
kable(effects)
```

## Let's work through it

```{r, echo = F}
effects$ACE = effects$Y_1 - effects$Y_0

kable(effects)
```

## Let's work through it

What is the average effect across all possible experiments?

```{r, echo = F}

kable(effects)
```

## Let's work through it


$$\frac{6 + 5 + 4 + 4 + 3 + 2}{6} = 4$$

## Let's work through it

```{r, echo =F}

hist(effects$ACE, xlab = "Estimated Causal Effects", main = "Outcomes of All Possible Experiments",
     breaks = seq(1.5, 6.5,1))
abline(v = 4, lwd = 4)

```

## Let's work through it

Why does this work?

If you wanted: 

- you can compare $Y^1$ for the treatment groups and $Y^1$ for the control to see if they are the same
- you can compare $Y^0$ for the control groups and $Y^0$ for the treatment to see if they are the same

## Let's work through it

```{r, echo =F, include = F}
effects$Y_1_0 = apply(assign, 1, function(x) mean(po_table$y1[-x]) )
effects$Y_0_1 = apply(assign, 1, function(x) mean(po_table$y0[x]))
```

Average of $Y^1$ for treated groups (factual outcome): `r mean(effects$Y_1)`

Average of $Y^1$ for control groups (counterfactual outcome): `r mean(effects$Y_1_0)`

#### 

Average of $Y^0$ for control groups (factual outcome): `r mean(effects$Y_0)`

Average of $Y^0$ for treated groups (counterfactual outcome): `r mean(effects$Y_0_1)`


## Randomized Experiments: {.build}

Using random assignment to treatment and control:

- Lets us have two samples that are, on average, counterfactuals for each other
- Lets us address FPCI at aggregate, not individual level

## Randomized Experiments: {.build}

### What happens when we fail to randomize?

1 and 2 always get treated, 3 and 4 never do.

| i | $Y_i^0$ | $Y_i^1$ | $Y_i^1 - Y_i^0$ | $X_i$
|---|---------|--------|-----------------|--|
| 1 | 5 | 9 | 4 |1|
| 2 | 4 | 8 | 4 |1|
| 3 | 3 | 7 | 4 |0|
| 4 | 2 | 6 | 4 |0|

## Randomized Experiments: {.build}

### What happens when we fail to randomize?

$$\frac{9+8}{2} - \frac{3+2}{2} = \frac{12}{2} \neq 4$$

## Randomized Experiments: {.build}

We can imagine cases with different potential outcomes have different attributes:

| i | $Y_i^0$ | $Y_i^1$ | $Y_i^1 - Y_i^0$ | $X_i$ | $W_i$ |
|---|---------|--------|-----------------|--|--|
| 1 | 5 | 9 | 4 |1|**1**|
| 2 | 4 | 8 | 4 |1|**1**|
| 3 | 3 | 7 | 4 |0|**0**|
| 4 | 2 | 6 | 4 |0|**0**|

## Randomized Experiments: {.build}

Presence of $W$ is fine if: $W$ unrelated to cause $X$

| i | $Y_i^0$ | $Y_i^1$ | $Y_i^1 - Y_i^0$ | $X_i$ | $W_i$ |
|---|---------|--------|-----------------|--|--|
| 1 | 3 | 7 | 4 |1|**1**|
| 2 | 2 | 6 | 4 |1|**0**|
| 3 | 3 | 7 | 4 |0|**1**|
| 4 | 2 | 6 | 4 |0|**0**|

## Randomized Experiments: {.build}

Presence of $W$ is fine if: $W$ unrelated to outcome $Y$

| i | $Y_i^0$ | $Y_i^1$ | $Y_i^1 - Y_i^0$ | $X_i$ | $W_i$ |
|---|---------|--------|-----------------|--|--|
| 1 | 3 | 7 | 4 |1|**1**|
| 2 | 2 | 6 | 4 |1|**1**|
| 3 | 3 | 7 | 4 |0|**0**|
| 4 | 2 | 6 | 4 |0|**0**|

## Randomized Experiments: {.build}

Randomization ensures that, on average, no relationship between $X$ and $W$.

### But things can go wrong

## Problems with Experiments:

### Internal Validity

A study has **internal validity** when the causal effect of $X$ on $Y$ it finds is not biased (systematically incorrect).

### Threats to internal validity

1. **Selection Bias**
2. **Non-excludability**
3. **Systematic Measurement Error**

## Threats to Internal Validity

### Selection Bias:

**selection bias**: When cases that receive a "treatment" or "cause" have different potential outcomes from those that do not.

## Threats to Internal Validity

### Selection Bias:

Consider this experiment:

- We want to evaluate the effect of emergency housing assistance on homelessness prevention.
- We give assistance to the first 100 people to show up and compare them to the second 100 people.

### Problem?

- People with greater need OR better resources/capabilities show up first
- Different needs or different resources probably related to propensity to become homeless
- Exposure to treatment related to other factors that might affect outcome!

## Threats to Internal Validity

### Non-excludability

**non-excludability** occurs when the treatment in an experiment bundles multiple different treatments

## Threats to Internal Validity

### Non-excludability

Does an apple a day keep the doctor away?

- Some people assigned to eat apples, some are not.
- To ensure people eat apples, we remind them every day to eat apples and give them a financial incentive to eat it.

### Problem?

- Are the effects we see due to apples? To being reminded every day? To being paid?
- Lots of differences between treatment and control. 

## Threats to Internal Validity

### Systematic Measurement Error

**systematic measurement error**: error produced when our measurement procedure obtains scores that are, on average, too high or too low.


## Threats to Internal Validity

### Systematic Measurement Error

We randomly assign some people to be shamed into voting (we remind them of their past voting record)

We measure whether people exposed to this treatment have higher **self-reported** voting rates

### Problem?

- If we see a difference between treatment and control, is it due to actual differences in voting?
- Or is it due to people who are shamed self-reporting differently?
- Treatment affects results of measurement procedures $\rightarrow$ measurement bias
- **Only** a problem **when** measurement bias is **related to treatment**

## Experiments:

### Randomization of cause

- Allows us to actually observe a counter-factual
- Directly tests causal theory
    - Allows us to see whether the change in $Y$ due to $X$ is in the right direction
    - Allows us to see whether $Y$ changes due to $X$ relative to the counterfactual.
    
### Problems are solvable

- Selection bias, non-excludability, measurement bias all solvable
- Better design of experiment

## Experiments: {.build}

One conclusion: only do experiments.

### But that could be a problem...

## Problems with Experiments:

### External Validity

**external validity** is the degree to which the causal relationship we find in a study matches the causal relationship and the context identified in a causal theory

- Study has **external validity** if the relationship found can generalize to all the cases to which our causal theory applies
    - If our study suffers from **sampling bias**, then our study may lack external validity
- Study has **external validity** if the cause in the study is the same as the cause in our causal theory
    - If our independent variable/cause in the study lacks **validity** then our study may lack external validity


## External Validity:

Broockman and Kalla (2015): Does perspective taking change minds about minority groups?

Sample

- Invite tens of thousands of people to join a survey
- Of the ~1500 that respond, randomly assign in two groups

Treatment

- One group gets canvasser for transgender rights
- Other gets canvasser for recycling

Outcome

- Survey responses on attitudes about transgender people, their rights

## Problems with Experiments: {.build}

### External Validity:

Broockman and Kalla (2015)

Sample

- **Invite tens of thousands** of people to join a survey
- **Of the ~1500 that respond**, randomly assign in two groups

### Problem:

- Will the effect of talking with canvasser hold for vast majority who won't join a survey?
- Sample in the experiment not the same as the population implied by the causal theory.

## Problems with Experiments:

### External Validity:

Does exposure to partisan media change people's political attitudes and make them more extreme? (E.g. Fox News, Breitbart)

Sample: Randomly chosen group of US adults

Treatment:

- Randomly exposed to 2 hours of partisan news or non-partisan movie

Outcome:

- Survey questions about political leaning, attitudes about key political issues

## Problems with Experiments:

### External Validity:

Treatment:

- Randomly **exposed to 2 hours** of partisan news or non-partisan movie

### Problem:

- Is this short treatment the same as years of exposure on cable TV or internet news consumption?
- If we find no effect of 2 hours of watching, does this mean that emergence of partisan news didn't cause change in Americans' political attitudes?
- Cause we can manipulate in experiment not the same as cause in causal theory

## Problems with Experiments:

### Always a Trade-off:

More **internal validity** (unbiased calculation of causal effect) comes at the cost in **external validity** (relevance of study sample or cause to the causal theory)

### Why?

Many relevant contexts for causal theories and interesting causes cannot or should not be manipulated at random:

- What causes democratization?
- What causes war or ethnic violence?
- What causes economic growth?
- Why were civil rights extended to oppressed minority groups?
    
## Problems with Experiments:

### We can't use experiments to text many causal theories

### What approaches can we use instead?

1. Comparative Method
2. Correlation

### Aren't they flawed?

Yes, but now that we know how and why experiments work, we can make these better.


<!-- ## Comparative Method -->

<!-- restate what it is -->

<!-- ## What do we know now? -->

<!-- - don't need ALL possible values to be the same -->
<!-- - Need to be the same on things that can affect both X and Y -->

<!-- ## Examples: -->


<!-- ## Conjunctural Causation: -->

<!-- example -->


<!-- ## Correlation -->

<!-- Suggests causation: -->

<!-- **correlation**: degree to which two variables vary **together** on average -->

<!-- What can it tell us about causal link? -->

<!-- - Causal theory says y moves with x. Should see them move together -->
<!-- - Should move in the direction suggested by the theory -->
<!-- - High X and High Y, or High X, Low Y -->

<!-- contrast to comparative method: -->

<!-- - look at correlations across more than two cases (usually a large number) -->
<!-- - can compare correlation at the same time **across units** -->
<!-- - can compare correlation within units **over time** -->

<!-- ## Correlation -->

<!-- correlation quartet -->


<!-- ## Correlation -->

<!-- Can observe clear relationships but low correlation -->


<!-- ## Correlation as test -->

<!-- examples -->

<!-- ## Correlation as test: -->

<!-- entire move from theory to evidence -->



<!-- ## Correlation: -->

<!-- empirical prediction is that x and y should be correlated -->

<!-- positive correlation -->
<!-- negative correlation -->

<!-- ## Correlation: -->

<!-- Typically: -->

<!-- We use one specific mathematical concept of correlation that implies moving together **linearly**. -->

<!-- This can be confusing, though... -->

<!-- ## Correlation -->

<!-- Weak -->

<!-- vs  -->

<!-- STrong -->

<!-- ## Correlation -->

<!-- examples: across units -->
<!-- - how correlation can tell us if there is something going on -->

<!-- ## Correlation -->

<!-- Exampes: across time -->

<!-- ## correlation  -->

<!-- randomness? -->

<!-- could we see a pattern that is purely by chance? -->

<!-- - appearance of correlation but is result of random noise -->

<!-- ##  -->

<!-- random relationships by chance -->

<!-- ## -->

<!-- statistics -->

<!-- probability of getting a relationship like this by chance, if there is no relationship -->

<!-- more cases, tighter relationship makes it less likely relationship is by chance -->

<!-- fewer cases, looser relationship more likely relationship is by chance -->

<!-- ## Statistical Significance -->

<!-- - how likely to get this by chance. -->

<!-- - higher significance means less likely we observe this by chance -->

<!-- ## P Value -->

<!-- a numerical measure of significance of a relationship: -->

<!-- - how likely is it that we would get this pattern by chance, ASSUMING NO RELATIONSHIP -->

<!-- low p value is better: often .05, but not magic. -->

<!-- - p value is useless if you do it many times... you will still BY CHANCE obtain a value of .05 or less. -->


<!-- ## Examples -->

<!-- - strong -->

<!-- - weak -->

<!-- - weak with low n -->

<!-- - stronger with larger n -->

<!-- - strong but high p bc not linear -->